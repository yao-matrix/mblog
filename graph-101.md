# å›¾æ™ºèƒ½ 101

## Why

### Graphæ— å¤„ä¸åœ¨

![Alt text|center](assets/graph-101/image-0.png)

### Graph Intelligence helps

![Alt text|center](assets/graph-101/image-1.png)

### It's the right time now!

Gartner é¢„æµ‹ï¼Œgraph æŠ€æœ¯åœ¨æ•°æ®å’Œåˆ†æåˆ›æ–°ä¸­çš„ä½¿ç”¨ç‡ä» 2021 å¹´çš„ 10%ï¼Œåˆ° 2025 å¹´ä¼šå¢é•¿åˆ° `80%`ã€‚ç›®å‰æ­£åœ¨ç»å†ä» `early adoption` åˆ° `early mainstream` çš„ç©¿è¶Šå¤§å³¡è°·æœŸé—´ï¼Œæ—¢ä¸å¤ªæ—©ä¹Ÿä¸å¤ªæ™šï¼Œæ—¶é—´åˆšåˆšå¥½ã€‚

![Alt text|center](assets/graph-101/image-2.png)

## What

### å¦‚ä½•å»ºæ¨¡å›¾

> A graph ğ’¢ is an ordered pair ğ’¢ = (ğ‘‰, ğ¸) comprising:
> - ğ‘‰, a set of vertices (or nodes)
> - ğ¸âŠ†{(ğ‘¥,ğ‘¦)|ğ‘¥,ğ‘¦âˆˆğ‘‰}, a set of edges (or links), which are pairs of nodes

> Example:
> 
>![Alt text](assets/graph-101/image-3.png)

#### Different Types of Graph

- Are edges directed?

	Directed Graph vs. Undirected Graph

- Are there multiple types of nodes or multiple types of edges?

	Homogeneous Graph vs Heterogeneous Graph

### å¦‚ä½•è¡¨ç¤ºå›¾

ä¸åŒçš„è¡¨ç¤ºæ–¹å¼ä¼šæŒ‡å‘ä¸åŒçš„è®¡ç®—æ¨¡å¼ã€‚

![Alt text](assets/graph-101/image-4.png)

### å¦‚ä½•è®¡ç®—å›¾

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå›¾çš„è®¡ç®—æ­¥éª¤å¦‚ä¸‹ï¼š

- éå†å›¾ä¸­çš„æ‰€æœ‰ç»“ç‚¹ï¼Œæˆ–è€…é‡‡æ ·å›¾ä¸­çš„ä¸€äº›ç»“ç‚¹ã€‚æ¯æ¬¡é€‰æ‹©å…¶ä¸­ä¸€ä¸ªç»“ç‚¹ï¼Œç§°ä¸ºç›®æ ‡ç»“ç‚¹ï¼ˆtarget nodeï¼‰ï¼› 
- ä¸€ä¸ª ğ¿-å±‚çš„ GNN è‡³å°‘éœ€è¦èšåˆç›®æ ‡ç»“ç‚¹çš„ L-è·³é‚»åŸŸçš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä»¥å›´ç»•ç›®æ ‡ç»“ç‚¹æ„é€ ä¸€ä¸ª L-è·³çš„ `ego-network`ã€‚å›¾ä¸­æ˜¯ä¸€ä¸ª 2-è·³ `ego-network` çš„ä¾‹å­ï¼Œå…¶ä¸­ç»¿è‰²ç»“ç‚¹æ˜¯ç¬¬ 1 è·³ï¼Œè“è‰²ç»“ç‚¹æ˜¯ç¬¬ 2 è·³ï¼›
- è®¡ç®—å¹¶æ›´æ–° `ego-network` é‡Œçš„æ¯ä¸ªç»“ç‚¹çš„ embeddingã€‚embedding ä¼šä½¿ç”¨åˆ°å›¾çš„ç»“æ„ä¿¡æ¯å’Œç»“ç‚¹ä¸è¾¹çš„ç‰¹å¾ã€‚

![Alt text|center](assets/graph-101/image-5.png)

é‚£ä¹ˆï¼Œè¿™äº› embedding æ˜¯å¦‚ä½•è®¡ç®—å’Œæ›´æ–°çš„å‘¢ï¼Ÿä¸»è¦æ˜¯ä½¿ç”¨ Message Passing çš„è®¡ç®—æ–¹æ³•ã€‚Message Passing æœ‰ä¸€äº›è®¡ç®—èŒƒå¼å¦‚ `GASï¼ˆGather-ApplyEdge-Scatterï¼‰`ï¼Œ`SAGAï¼ˆScatter-ApplyEdge-Gather-ApplyVertexï¼‰`ç­‰ã€‚æˆ‘ä»¬è¿™é‡Œä»‹ç»å½’çº³å¾—æ¯”è¾ƒå…¨é¢çš„ `SAGA` è®¡ç®—èŒƒå¼ã€‚å‡è®¾éœ€è¦è®¡ç®—å’Œæ›´æ–°ä¸‹å›¾ä¸­çš„ $\vec{x_1}$:

![Alt text|center](assets/graph-101/image-6.png)

- **`Scatter`**

	Propagate message from source vertices to edge.

	![Alt text|center](assets/graph-101/image-7.png)

- **`ApplyEdge`**

	Transform message along each edge.

	![Alt text|center](assets/graph-101/image-8.png)

- **`Gather`**

	Gather transformed message to the destination vertex.

	![Alt text|center](assets/graph-101/image-9.png)

- **`ApplyVertex`**

	Transform the gathered output to get updated vertex.

	![Alt text|center](assets/graph-101/image-10.png)

å…¬å¼å¦‚ä¸‹ï¼š

![Alt text|center](assets/graph-101/image-11.png)

åˆ†æä¸€ä¸‹ä¼šå‘ç°ï¼Œ`SAGA` æ¨¡å¼ä¸­ `ApplyEdge` å’Œ `ApplyVertex` æ˜¯ä¼ ç»Ÿ deep learning ä¸­çš„ NNï¼ˆNeural Networkï¼‰æ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥å¤ç”¨ï¼›è€Œ `Scatter` å’Œ `Gather` æ˜¯ GNN æ–°å¼•å…¥çš„æ“ä½œã€‚å³ï¼š**Graph Computing = Graph Ops + NN Ops**ã€‚

![Alt text|center](assets/graph-101/image-12.png)

### ä¸åŒçš„å›¾æ•°æ®é›†è§„æ¨¡

- One big graph

	å¯èƒ½é«˜è¾¾æ•°åäº¿çš„ç»“ç‚¹ï¼Œæ•°ç™¾äº¿çš„è¾¹ã€‚

	![Alt text|center](assets/graph-101/image-13.png)

- Many small graphs

	![Alt text|center](assets/graph-101/image-14.png)

### ä¸åŒçš„å›¾ä»»åŠ¡

- Node-level prediction

	é¢„æµ‹å›¾ä¸­ç»“ç‚¹çš„ç±»åˆ«æˆ–æ€§è´¨

	![Alt text|center](assets/graph-101/image-15.png)

- Edge-level prediction

	é¢„æµ‹å›¾ä¸­ä¸¤ä¸ªç»“ç‚¹æ˜¯å¦å­˜åœ¨è¾¹ï¼Œä»¥åŠè¾¹çš„ç±»åˆ«æˆ–æ€§è´¨

	![Alt text|center](assets/graph-101/image-16.png)

- Graph-level prediction

	é¢„æµ‹æ•´å›¾æˆ–å­å›¾çš„ç±»åˆ«æˆ–æ€§è´¨

	![Alt text|center](assets/graph-101/image-17.png)

## How

### Workflow

![Alt text|center](assets/graph-101/image-18.png)

> ä»¥ fraud detection ä¸ºä¾‹ï¼š
>	- Tabformer æ•°æ®é›†
>	![Alt text](assets/graph-101/image-19.png)
>	- workflow
>   ![Alt text](assets/graph-101/image-20.png)

### è½¯ä»¶æ ˆ

![Alt text|center](assets/graph-101/image-21.png)

- è®¡ç®—å¹³é¢

	![Alt text|center](assets/graph-101/image-22.png)

- æ•°æ®å¹³é¢

	![Alt text|center](assets/graph-101/image-23.png)

### SW Challenges

#### Graph Sampler

For `many small graphs` datasets, **full batch training** works most time. Full batch training means we can do training on whole graph;

When it comes to `one large graph` datasets, in many real scenarios, we meet `Neighbor Explosion` problem;  

> Neighbor Explosion:
> 
> ![Alt text|center](assets/graph-101/image-24.png)

Graph sampler comes to rescue. Only sample a fraction of target nodes, and furthermore, for each target node, we sample a sub-graph of its ego-network for training. This is called **mini-batch training**.
Graph sampling is triggered for each data loading.  And the hops of the sampled graph equals the GNN layer number ğ¿. Which means graph sampler in data loader is important in GNN training.

![Alt text|center](assets/graph-101/image-25.png)

**Challenge: How to optimize sampler both as standalone and in training pipe?**

When graph comes to huge(billions of nodes, tens of billions of edges), we meet new at-scale challenges:
- How to store the huge graph across node? -> graph partition
- How to build a training system w/ not only distributed model computing but also distributed graph store and sampling?
	- How to cut the graph while minimize cross partition connections?

		![Alt text|center|200x0](assets/graph-101/image-26.png)

A possible GNN distributed training architecture:

![Alt text|center](assets/graph-101/image-27.png)

#### Scatter-Gather
- Fuse adjacent graphs ops
  
	> One common fuse pattern for GCN & GraphSAGEï¼š
	>
	>	![Alt text|center](assets/graph-101/image-28.png)
	
	**Challenge:**  
	**How to fuse more GNN patterns on different ApplyEdge and ApplyVertex, automatically?**

- How to implement fused Aggregate
  
	![Alt text|center](assets/graph-101/image-29.png)

    **Challenge:**
	- **Different graph data structures lead to different implementations in same logic operations;**
	- **Different graph characteristics favors different data structures;(like low-degree graphs favor COO, high-degree graphs favor CSR)**
	- **How to find the applicable zone for each and hide such complexity to data scientists?**

#### More

- Inference challenge
	- GNN inference needs full batch inference, how to make it efficient?
	- Distributed inference for big graph?
	- Vector quantization for node and edge features? 
	- GNN distilled to MLP?
- SW-HW co-design challenge
	- How to relief irregular memory access in scatter-gather?
	- Do we need some data flow engine for acceleration?
-  â€¦

## Finishing words

"There is plenty of room at the top" å¯¹æŠ€æœ¯äººå‘˜å¾ˆé‡è¦ã€‚ä½†ä¸ºé¿å…å…¥å®å±±è€Œç©ºè¿”ï¼Œæˆ‘ä»¬æ›´éœ€è¦å»ºç«‹èµ·æŠ€æœ¯æ¶æ„ï¼Œè¿™å°±åƒæ˜¯åœ°å›¾ä¸€æ ·ï¼Œåªæœ‰æŒ‰å›¾ç´¢éª¥æ‰èƒ½æ›´å¥½åœ°æ¢ç´¢å’Œåˆ©ç”¨å¥½ top é‡Œçš„ `plenty of room`ã€‚

![Alt text](assets/graph-101/image-30.png)

## References

1. [Graph + AI: What's Next? Progress in Democratizing Graph for All](https://www.graphaisummit.com/agenda/session/895480)
2. [Recent Advances in Efficient and Scalable Graph Neural Networks](https://www.chaitjo.com/post/efficient-gnns/#learning-paradigms-for-gnn-compression)
3. [Crossing the Chasm â€“ Technology adoption lifecycle](https://thinkinsights.net/strategy/crossing-the-chasm/)
4. [Understanding and Bridging the Gaps in Current GNN Performance Optimizations](https://dl.acm.org/doi/pdf/10.1145/3437801.3441585)
5. [Automatic Generation of High-Performance Inference Kernels for Graph Neural Networks on Multi-Core Systems](https://dl.acm.org/doi/fullHtml/10.1145/3472456.3473511)
6. [Understanding GNN Computational Graph: A Coordinated Computation, IO, And Memory Perspective](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2Fe96e4419-630b-4bf7-8635-61256efcfbd4.pdf)
7. [Graphiler: A Compiler For Graph Neural Networks](https://gnnsys.github.io/papers/GNNSys21_paper_10.pdf)
8. [Scatter-Add in Data Parallel Architectures](http://cva.stanford.edu/people/gajh/publications/hpca11_camera.pdf)
9. [fuseGNN: Accelerating Graph Convolutional Neural Network Training on GPGPU](https://seal.ece.ucsb.edu/sites/default/files/publications/fusegcn_camera_ready_.pdf)
10. [VQ-GNN: A Universal Framework to Scale up Graph Neural Networks using Vector Quantization](https://neurips.cc/media/neurips-2021/Slides/26298.pdf)
11. [NeuGraph: Parallel Deep Neural Network Computation on Large Graphs](https://www.usenix.org/conference/atc19/presentation/ma)
12. [Completing a member knowledge graph with Graph Neural Networks](https://engineering.linkedin.com/blog/2021/completing-a-member-knowledge-graph-with-graph-neural-networks)
13. [PinnerFormer: Sequence Modeling for User Representation at Pinterest](https://arxiv.org/pdf/2205.04507.pdf)
14. [Gartner and Graph Analytics](https://graphlytic.biz/blog/gartner-and-graph-analytics)
15. [Analysis and Optimization of GNN-Based Recommender Systems on Persistent Memory](https://arxiv.org/pdf/2207.11918.pdf)
16. [GNNear: Accelerating Full-Batch Training of Graph Neural Networks with Near-Memory Processing](https://arxiv.org/pdf/2111.00680.pdf)
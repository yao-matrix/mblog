---
title: "ä½¿ç”¨ FHE å®ç°åŠ å¯†å¤§è¯­è¨€æ¨¡å‹" 
thumbnail: /blog/assets/encrypted-llm/thumbnail.png
authors:
- user: RomanBredehoft
  guest: true
- user: jfrery-zama
  guest: true
translators:
- user: MatrixYao
---

# ä½¿ç”¨ FHE å®ç°åŠ å¯†å¤§è¯­è¨€æ¨¡å‹

<!-- {blog_metadata} -->
<!-- {authors} -->

è¿‘æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ (LLM) å·²è¢«è¯æ˜æ˜¯æé«˜ç¼–ç¨‹ã€å†…å®¹ç”Ÿæˆã€æ–‡æœ¬åˆ†æã€ç½‘ç»œæœç´¢åŠè¿œç¨‹å­¦ä¹ ç­‰è¯¸å¤šé¢†åŸŸç”Ÿäº§åŠ›çš„å¯é å·¥å…·ã€‚

## å¤§è¯­è¨€æ¨¡å‹å¯¹ç”¨æˆ·éšç§çš„å½±å“

å°½ç®¡ LLM å¾ˆæœ‰å¸å¼•åŠ›ï¼Œä½†å¦‚ä½•ä¿æŠ¤å¥½`è¾“å…¥ç»™è¿™äº›æ¨¡å‹çš„ç”¨æˆ·æŸ¥è¯¢ä¸­çš„éšç§`è¿™ä¸€é—®é¢˜ä»ç„¶å­˜åœ¨ã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬æƒ³å……åˆ†åˆ©ç”¨ LLM çš„åŠ›é‡ï¼Œä½†å¦ä¸€æ–¹é¢ï¼Œå­˜åœ¨å‘ LLM æœåŠ¡æä¾›å•†æ³„éœ²æ•æ„Ÿä¿¡æ¯çš„é£é™©ã€‚åœ¨æŸäº›é¢†åŸŸï¼Œä¾‹å¦‚åŒ»ç–—ä¿å¥ã€é‡‘èæˆ–æ³•å¾‹ï¼Œè¿™ç§éšç§é£é™©ç”šè‡³æœ‰ä¸€ç¥¨å¦å†³æƒã€‚

ä¸€ç§å¤‡é€‰è§£å†³æ–¹æ¡ˆæ˜¯æœ¬åœ°åŒ–éƒ¨ç½²ï¼ŒLLM æ‰€æœ‰è€…å°†å…¶æ¨¡å‹éƒ¨ç½²åœ¨å®¢æˆ·çš„è®¡ç®—æœºä¸Šã€‚ç„¶è€Œï¼Œè¿™ä¸æ˜¯æœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºæ„å»º LLM å¯èƒ½éœ€è¦èŠ±è´¹æ•°ç™¾ä¸‡ç¾å…ƒï¼ˆ[GPT3 ä¸º 460 ä¸‡ç¾å…ƒ](https://lambdalabs.com/blog/demystifying-gpt-3)ï¼‰ï¼Œè€Œæœ¬åœ°éƒ¨ç½²æœ‰æ³„éœ²æ¨¡å‹çŸ¥è¯†äº§æƒï¼ˆintellectual property, IPï¼‰çš„é£é™©ã€‚

Zama ç›¸ä¿¡æœ‰ä¸¤å…¨å…¶ç¾ä¹‹æ³•ï¼šæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŒæ—¶ä¿æŠ¤ç”¨æˆ·çš„éšç§å’Œæ¨¡å‹çš„ IPã€‚é€šè¿‡æœ¬æ–‡ï¼Œä½ å°†äº†è§£å¦‚ä½•åˆ©ç”¨ Hugging Face transformers åº“å¹¶è®©è¿™äº›æ¨¡å‹çš„æŸäº›éƒ¨åˆ†åœ¨åŠ å¯†æ•°æ®ä¸Šè¿è¡Œã€‚å®Œæ•´ä»£ç è§[æ­¤å¤„](https://github.com/zama-ai/concrete-ml/tree/17779ca571d20b001caff5792eb11e76fe2c19ba/use_case_examples/llm)ã€‚

## å…¨åŒæ€åŠ å¯† (Fully Homomorphic Encryptionï¼ŒFHE) å¯ä»¥è§£å†³ LLM éšç§æŒ‘æˆ˜

é’ˆå¯¹ LLM éƒ¨ç½²çš„éšç§æŒ‘æˆ˜ï¼ŒZama çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨å…¨åŒæ€åŠ å¯† (FHE)ï¼Œåœ¨åŠ å¯†æ•°æ®ä¸Šæ‰§è¡Œå‡½æ•°ã€‚è¿™ç§åšæ³•å¯ä»¥å®ç°ä¸¤éš¾è‡ªè§£ï¼Œæ—¢å¯ä»¥ä¿æŠ¤æ¨¡å‹æ‰€æœ‰è€…çŸ¥è¯†äº§æƒï¼ŒåŒæ—¶åˆèƒ½ç»´æŠ¤ç”¨æˆ·çš„æ•°æ®éšç§ã€‚æˆ‘ä»¬çš„æ¼”ç¤ºè¡¨æ˜ï¼Œåœ¨ FHE ä¸­å®ç°çš„ LLM æ¨¡å‹ä¿æŒäº†åŸå§‹æ¨¡å‹çš„é¢„æµ‹è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´ Hugging Face [transformers åº“](https://github.com/huggingface/transformers)ä¸­çš„ [GPT2](https://huggingface.co/gpt2) å®ç°ï¼Œä½¿ç”¨ Concrete-Python å¯¹æ¨ç†éƒ¨åˆ†è¿›è¡Œæ”¹é€ ï¼Œè¿™æ ·å°±å¯ä»¥å°† Python å‡½æ•°è½¬æ¢ä¸ºå…¶ FHE ç­‰æ•ˆå‡½æ•°ã€‚

![å›¾ 1. GPT2 æ¶æ„ï¼›å›¾æºï¼šhttps://en.wikipedia.org/wiki/GPT-2](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/encrypted-llm/gpt2_architecture.png)

å›¾ 1 å±•ç¤ºäº†ç”±å¤šä¸ª transformer block å †å è€Œæˆçš„ GPT2 æ¶æ„ï¼šå…¶ä¸­æœ€ä¸»è¦çš„æ˜¯å¤šå¤´æ³¨æ„åŠ› (multi-head attentionï¼ŒMHA) å±‚ã€‚æ¯ä¸ª MHA å±‚ä½¿ç”¨æ¨¡å‹æƒé‡æ¥å¯¹è¾“å…¥è¿›è¡ŒæŠ•å½±ï¼Œç„¶åå„è‡ªè®¡ç®—æ³¨æ„åŠ›ï¼Œå¹¶å°†æ³¨æ„åŠ›çš„è¾“å‡ºé‡æ–°æŠ•å½±åˆ°æ–°çš„å¼ é‡ä¸­ã€‚

åœ¨ [TFHE](https://www.zama.ai/post/tfhe-deep-dive-part-1) ä¸­ï¼Œæ¨¡å‹æƒé‡å’Œæ¿€æ´»å‡ç”¨æ•´æ•°è¡¨ç¤ºã€‚éçº¿æ€§å‡½æ•°å¿…é¡»é€šè¿‡å¯ç¼–ç¨‹è‡ªä¸¾ (Programmable Bootstrappingï¼ŒPBS) æ“ä½œæ¥å®ç°ã€‚PBS å¯¹åŠ å¯†æ•°æ®å®æ–½æŸ¥è¡¨ (table lookupï¼ŒTLU) æ“ä½œï¼ŒåŒæ—¶åˆ·æ–°å¯†æ–‡ä»¥æ”¯æŒ[ä»»æ„è®¡ç®—](https://whitepaper.zama.ai/)ã€‚ä¸å¥½çš„ä¸€é¢æ˜¯ï¼Œæ­¤æ—¶ PBS çš„è®¡ç®—æ—¶é—´åœ¨çº¿æ€§è¿ç®—ä¸­å ä¸»å¯¼åœ°ä½ã€‚åˆ©ç”¨è¿™ä¸¤ç§ç±»å‹çš„è¿ç®—ï¼Œä½ å¯ä»¥åœ¨ FHE ä¸­è¡¨è¾¾ä»»ä½•å­æ¨¡å‹çš„è®¡ç®—ï¼Œç”šè‡³å®Œæ•´çš„ LLM è®¡ç®—ã€‚

## ä½¿ç”¨ FHE å®ç° LLM çš„ä¸€å±‚

æ¥ä¸‹æ¥ï¼Œä½ å°†äº†è§£å¦‚ä½•åŠ å¯†å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰ä¸­çš„ä¸€ä¸ªæ³¨æ„åŠ›å¤´ã€‚ä½ å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/zama-ai/concrete-ml/tree/17779ca571d20b001caff5792eb11e76fe2c19ba/use_case_examples/llm)æ‰¾åˆ°å®Œæ•´çš„ MHA å®ç°ä»£ç ã€‚

![å›¾ 2. åœ¨ FHE ä¸­è¿è¡Œ LLM æ¨¡å‹çš„æŸäº›éƒ¨åˆ†](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/encrypted-llm/hybrid_gpt2_visualisation.svg)

å›¾ 2 æ¦‚è¿°äº†ä¸€ä¸ªç®€åŒ–çš„åº•å±‚å®ç°ã€‚åœ¨è¿™ä¸ªæ–¹æ¡ˆä¸­ï¼Œæ¨¡å‹æƒé‡ä¼šè¢«åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«å­˜å‚¨åœ¨å®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯ã€‚é¦–å…ˆï¼Œå®¢æˆ·ç«¯åœ¨æœ¬åœ°å¼€å§‹æ¨ç†ï¼Œç›´è‡³é‡åˆ°å·²ç¬¬ä¸€ä¸ªä¸åœ¨æœ¬åœ°çš„å±‚ã€‚ç”¨æˆ·å°†ä¸­é—´ç»“æœåŠ å¯†å¹¶å‘é€ç»™æœåŠ¡ç«¯ã€‚æœåŠ¡ç«¯å¯¹å…¶æ‰§è¡Œç›¸åº”çš„æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—ï¼Œç„¶åå°†ç»“æœè¿”å›ç»™å®¢æˆ·ç«¯ï¼Œå®¢æˆ·ç«¯å¯¹ç»“æœè¿›è¡Œè§£å¯†å¹¶ç»§ç»­åœ¨æœ¬åœ°æ¨ç†ã€‚

### é‡åŒ–

é¦–å…ˆï¼Œä¸ºäº†å¯¹åŠ å¯†å€¼è¿›è¡Œæ¨¡å‹æ¨ç†ï¼Œæ¨¡å‹çš„æƒé‡å’Œæ¿€æ´»å¿…é¡»è¢«é‡åŒ–å¹¶è½¬æ¢ä¸ºæ•´æ•°ã€‚ç†æƒ³æƒ…å†µæ˜¯ä½¿ç”¨[è®­ç»ƒåé‡åŒ–](https://docs.zama.ai/concrete-ml/advanced-topics/quantization)ï¼Œè¿™æ ·å°±ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹äº†ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨æ•´æ•°å’Œ PBS æ¥å®ç° FHE å…¼å®¹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶æ£€æŸ¥å…¶å¯¹ LLM å‡†ç¡®ç‡çš„å½±å“ã€‚

è¦è¯„ä¼°é‡åŒ–çš„å½±å“ï¼Œæˆ‘ä»¬è¿è¡Œå®Œæ•´çš„ GPT2 æ¨¡å‹ï¼Œå¹¶è®©å…¶ä¸­çš„ä¸€ä¸ª LLM å¤´è¿›è¡Œå¯†æ€è®¡ç®—ã€‚ç„¶åæˆ‘ä»¬åŸºäºæ­¤è¯„ä¼°æƒé‡å’Œæ¿€æ´»çš„é‡åŒ–æ¯”ç‰¹æ•°å¯¹å‡†ç¡®ç‡çš„å½±å“ã€‚

![å•æ³¨æ„åŠ›å¤´é‡åŒ–çš„å¹³å‡ top-k å‡†ç¡®ç‡](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/encrypted-llm/qattention_accuracy.png)

ä¸Šå›¾è¡¨æ˜ 4 æ¯”ç‰¹é‡åŒ–ä¿æŒäº†åŸå§‹ç²¾åº¦çš„ 96%ã€‚è¯¥å®éªŒåŸºäºå«æœ‰çº¦ 80 ä¸ªå¥å­çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡å°†åŸå§‹æ¨¡å‹çš„ logits é¢„æµ‹ä¸å¸¦æœ‰é‡åŒ–æ³¨æ„åŠ›å¤´çš„æ¨¡å‹çš„ logits é¢„æµ‹è¿›è¡Œæ¯”è¾ƒæ¥è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ã€‚

### åœ¨ Hugging Face GPT2 æ¨¡å‹ä¸­ä½¿ç”¨ FHE

æˆ‘ä»¬éœ€è¦åœ¨ Hugging Face çš„ transformers åº“çš„åŸºç¡€ä¸Šé‡å†™åŠ å¯†æ¨¡å—çš„å‰å‘ä¼ æ’­ï¼Œä»¥ä½¿å…¶åŒ…å«é‡åŒ–ç®—å­ã€‚é¦–å…ˆé€šè¿‡åŠ è½½ [GPT2LMHeadModel](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel) æ„å»ºä¸€ä¸ª SingleHeadQGPT2Model å®ä¾‹ï¼Œç„¶åæ‰‹åŠ¨ä½¿ç”¨ [QGPT2SingleHeadAttention](https://github.com/zama-ai/concrete-ml/blob/c291399cb1f2a0655c308c14e2180eb2ffda0ab7/use_case_examples/llm/qgpt2_models.py#L191) æ›¿æ¢ç¬¬ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›æ¨¡å—ï¼Œä»£ç å¦‚ä¸‹ã€‚ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/zama-ai/concrete-ml/blob/c291399cb1f2a0655c308c14e2180eb2ffda0ab7/use_case_examples/llm/qgpt2_models.py)æ‰¾åˆ°æ¨¡å‹çš„å®Œæ•´å®ç°ã€‚

```python
self.transformer.h[0].attn = QGPT2SingleHeadAttention(config, n_bits=n_bits)
```

è‡³æ­¤ï¼Œå‰å‘ä¼ æ’­å·²è¢«é‡è½½æˆç”¨ FHE ç®—å­å»æ‰§è¡Œå¤šå¤´æ³¨æ„åŠ›çš„ç¬¬ä¸€ä¸ªå¤´ï¼ŒåŒ…æ‹¬æ„å»ºæŸ¥è¯¢ã€é”®å’Œå€¼çŸ©é˜µçš„æŠ•å½±ã€‚ä»¥ä¸‹ä»£ç ä¸­çš„ `QGPT2` æ¨¡å—çš„ä»£ç è§[æ­¤å¤„](https://github.com/zama-ai/concrete-ml/blob/c291399cb1f2a0655c308c14e2180eb2ffda0ab7/use_case_examples/llm/qgpt2_class.py#L196)ã€‚

```python
class SingleHeadAttention(QGPT2):
    """Class representing a single attention head implemented with quantization methods."""


    def run_numpy(self, q_hidden_states: np.ndarray):

        # Convert the input to a DualArray instance
        q_x = DualArray(
            float_array=self.x_calib,
            int_array=q_hidden_states,
            quantizer=self.quantizer
        )

        # Extract the attention base module name
        mha_weights_name = f"transformer.h.{self.layer}.attn."

        # Extract the query, key and value weight and bias values using the proper indices
        head_0_indices = [
            list(range(i * self.n_embd, i * self.n_embd + self.head_dim)) 
            for i in range(3)
        ]
        q_qkv_weights = ...
        q_qkv_bias = ...

        # Apply the first projection in order to extract Q, K and V as a single array
        q_qkv = q_x.linear(
            weight=q_qkv_weights,
            bias=q_qkv_bias,
            key=f"attention_qkv_proj_layer_{self.layer}",
        )

        # Extract the queries, keys and vales
        q_qkv = q_qkv.expand_dims(axis=1, key=f"unsqueeze_{self.layer}")
        q_q, q_k, q_v = q_qkv.enc_split(
            3, 
            axis=-1, 
            key=f"qkv_split_layer_{self.layer}"
        )

        # Compute attention mechanism
        q_y = self.attention(q_q, q_k, q_v)

        return self.finalize(q_y)
```

æ¨¡å‹ä¸­çš„å…¶ä»–è®¡ç®—ä»ä»¥æµ®ç‚¹å½¢å¼è¿›è¡Œï¼ŒæœªåŠ å¯†ï¼Œå¹¶ç”±å®¢æˆ·ç«¯åœ¨æœ¬åœ°æ‰§è¡Œã€‚

å°†é¢„è®­ç»ƒçš„æƒé‡åŠ è½½åˆ°ä¿®æ”¹åçš„ GPT2 æ¨¡å‹ä¸­ï¼Œç„¶åè°ƒç”¨ _generate_ æ–¹æ³•ï¼š

```python
qgpt2_model = SingleHeadQGPT2Model.from_pretrained(
    "gpt2_model", n_bits=4, use_cache=False
)


output_ids = qgpt2_model.generate(input_ids)
```

ä¸¾ä¸ªä¾‹å­ï¼Œä½ å¯ä»¥è¦æ±‚é‡åŒ–æ¨¡å‹è¡¥å…¨çŸ­è¯­ â€œCryptography is aâ€ ã€‚åœ¨ FHE ä¸­è¿è¡Œæ¨¡å‹æ—¶ï¼Œå¦‚æœé‡åŒ–ç²¾åº¦è¶³å¤Ÿï¼Œç”Ÿæˆçš„è¾“å‡ºä¸ºï¼š

â€œCryptography is a very important part of the security of your computerâ€

å½“é‡åŒ–ç²¾åº¦å¤ªä½æ—¶ï¼Œæ‚¨ä¼šå¾—åˆ°ï¼š

â€œCryptography is a great way to learn about the world around youâ€

### ç¼–è¯‘ä¸º FHE

ç°åœ¨ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ Concrete-ML ä»£ç ç¼–è¯‘æ³¨æ„åŠ›å¤´ï¼š

```python
circuit_head = qgpt2_model.compile(input_ids)
```

è¿è¡Œæ­¤ä»£ç ï¼Œä½ å°†çœ‹åˆ°ä»¥ä¸‹æ‰“å°è¾“å‡ºï¼šâ€œCircuit compiled with 8 bit-widthâ€ã€‚è¯¥é…ç½®ä¸ FHE å…¼å®¹ï¼Œæ˜¾ç¤ºäº†åœ¨ FHE ä¸­æ‰§è¡Œçš„æ“ä½œæ‰€éœ€çš„æœ€å¤§ä½å®½ã€‚

### å¤æ‚åº¦

åœ¨ transformer æ¨¡å‹ä¸­ï¼Œè®¡ç®—é‡æœ€å¤§çš„æ“ä½œæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒå°†æŸ¥è¯¢ã€é”®å’Œå€¼ç›¸ä¹˜ã€‚åœ¨ FHE ä¸­ï¼ŒåŠ å¯†åŸŸä¸­ä¹˜æ³•çš„ç‰¹æ®Šæ€§åŠ å‰§äº†æˆæœ¬ã€‚æ­¤å¤–ï¼Œéšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œè¿™äº›ä¹˜æ³•çš„æ•°é‡è¿˜ä¼šå‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚

è€Œå°±åŠ å¯†æ³¨æ„åŠ›å¤´è€Œè¨€ï¼Œé•¿åº¦ä¸º 6 çš„åºåˆ—éœ€è¦ 11622 æ¬¡ PBS æ“ä½œã€‚æˆ‘ä»¬ç›®å‰çš„å®éªŒè¿˜å¾ˆåˆæ­¥ï¼Œå°šæœªå¯¹æ€§èƒ½è¿›è¡Œä¼˜åŒ–ã€‚è™½ç„¶å¯ä»¥åœ¨å‡ ç§’é’Ÿå†…è¿è¡Œï¼Œä½†ä¸å¯å¦è®¤å®ƒéœ€è¦ç›¸å½“å¤šçš„è®¡ç®—èƒ½åŠ›ã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬é¢„æœŸï¼Œå‡ å¹´åï¼Œç¡¬ä»¶ä¼šå°†å»¶è¿Ÿæé«˜ 1000 å€åˆ° 10000 å€ï¼Œä½¿åŸæ¥åœ¨ CPU ä¸Šéœ€è¦å‡ åˆ†é’Ÿçš„æ“ä½œç¼©çŸ­åˆ° ASIC ä¸Šçš„ä½äº 100 æ¯«ç§’ã€‚æœ‰å…³è¿™äº›ä¼°ç®—çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[æ­¤åšæ–‡](https://www.zama.ai/post/chatgpt-privacy-with-homomorphic-encryption)ã€‚

## æ€»ç»“

å¤§è¯­è¨€æ¨¡å‹æœ‰æœ›ä½¿èƒ½å¤§é‡åº”ç”¨åœºæ™¯ï¼Œä½†å…¶å®ç°å¼•å‘äº†ç”¨æˆ·éšç§çš„é‡å¤§å…³åˆ‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æœç€å¯†æ€ LLM è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆæ„¿æ™¯æ˜¯è®©æ•´ä¸ªæ¨¡å‹å®Œå…¨åœ¨äº‘ä¸Šè¿è¡Œï¼ŒåŒæ—¶ç”¨æˆ·çš„éšç§è¿˜èƒ½å¾—åˆ°å……åˆ†å°Šé‡ã€‚

å½“å‰çš„åšæ³•åŒ…æ‹¬å°† GPT2 ç­‰æ¨¡å‹ä¸­çš„ç‰¹å®šéƒ¨åˆ†è½¬æ¢è‡³ FHE åŸŸã€‚æˆ‘ä»¬çš„å®ç°åˆ©ç”¨äº† transformers åº“ï¼Œç”¨æˆ·è¿˜èƒ½è¯„ä¼°æ¨¡å‹çš„ä¸€éƒ¨åˆ†åœ¨åŠ å¯†æ•°æ®ä¸Šè¿è¡Œæ—¶å¯¹å‡†ç¡®ç‡çš„å½±å“ã€‚é™¤äº†ä¿æŠ¤ç”¨æˆ·éšç§ä¹‹å¤–ï¼Œè¿™ç§æ–¹æ³•è¿˜å…è®¸æ¨¡å‹æ‰€æœ‰è€…å¯¹å…¶æ¨¡å‹çš„ä¸»è¦éƒ¨åˆ†ä¿å¯†ã€‚ä½ å¯åœ¨[æ­¤å¤„](https://github.com/zama-ai/concrete-ml/tree/17779ca571d20b001caff5792eb11e76fe2c19ba/use_case_examples/llm)æ‰¾åˆ°å®Œæ•´ä»£ç ã€‚

Zama åº“ [Concrete](https://github.com/zama-ai/concrete) å’Œ [Concrete-ML](https://github.com/zama-ai/concrete-ml)ï¼ˆåˆ«å¿˜äº†ç»™æˆ‘ä»¬çš„ github ä»£ç åº“ç‚¹ä¸ªæ˜Ÿæ˜Ÿ â­ï¸ğŸ’›ï¼‰å…è®¸ç›´æ¥æ„å»º ML æ¨¡å‹å¹¶å°†å…¶è½¬æ¢è‡³ç­‰ä»·çš„ FHE åŸŸï¼Œä»è€Œä½¿ä¹‹èƒ½å¤Ÿå¯¹åŠ å¯†æ•°æ®è¿›è¡Œè®¡ç®—å’Œé¢„æµ‹ã€‚

å¸Œæœ›ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ã€‚è¯·éšæ—¶åˆ†äº«ä½ çš„æƒ³æ³•/åé¦ˆï¼

> è‹±æ–‡åŸæ–‡: <url> https://huggingface.co/blog/encrypted-llm </url>
> åŸæ–‡ä½œè€…ï¼šRoman Bredehoftï¼ŒJordan Frery
> è¯‘è€…: Matrix Yao (å§šä¼Ÿå³°)ï¼Œè‹±ç‰¹å°”æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œå·¥ä½œæ–¹å‘ä¸º transformer-family æ¨¡å‹åœ¨å„æ¨¡æ€æ•°æ®ä¸Šçš„åº”ç”¨åŠå¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒæ¨ç†ã€‚

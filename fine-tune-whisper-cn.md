---
title: "ä½¿ç”¨ ğŸ¤— Transformers ä¸ºå¤šè¯­ç§è¯­éŸ³è¯†åˆ«ä»»åŠ¡å¾®è°ƒ Whisper æ¨¡å‹" 
thumbnail: /blog/assets/111_fine_tune_whisper/thumbnail.jpg
authors:
- user: sanchit-gandhi
translators:
- user: MatrixYao
---

# ä½¿ç”¨ ğŸ¤— Transformers ä¸ºå¤šè¯­ç§è¯­éŸ³è¯†åˆ«ä»»åŠ¡å¾®è°ƒ Whisper æ¨¡å‹

<!-- {blog_metadata} -->
<!-- {authors} -->

<a target="_blank" href="https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="åœ¨ Colab ä¸­æ‰“å¼€"/>
</a>

é€šè¿‡æœ¬æ–‡ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä½¿ç”¨ Hugging Face ğŸ¤— Transformers åœ¨ä»»æ„å¤šè¯­ç§è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ•°æ®é›†ä¸Šå¾®è°ƒ Whisper çš„åˆ†æ­¥æŒ‡å—ã€‚æˆ‘ä»¬è¿˜æ·±å…¥è§£é‡Šäº† Whisper æ¨¡å‹ã€Common Voice æ•°æ®é›†ã€å¾®è°ƒèƒŒåçš„ç†è®ºï¼ŒåŒæ—¶æˆ‘ä»¬è¿˜æä¾›äº†ç”¨äºå‡†å¤‡æ•°æ®å’Œå¾®è°ƒçš„ç›¸åº”ä»£ç ã€‚å¦‚æœä½ æƒ³è¦ä¸€ä¸ªå…¨ä»£ç ã€è§£é‡Šå°‘çš„ Notebookï¼Œå¯ä»¥å‚é˜…è¿™ä¸ª [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb)ã€‚

## ç›®å½•
1. [ç®€ä»‹](#ç®€ä»‹)
2. [åœ¨ Google Colab ä¸­å¾®è°ƒ Whisper](#åœ¨-google-colab-ä¸­å¾®è°ƒ-whisper)
    1. [å‡†å¤‡ç¯å¢ƒ](#å‡†å¤‡ç¯å¢ƒ)
    2. [åŠ è½½æ•°æ®é›†](#åŠ è½½æ•°æ®é›†)
    3. [å‡†å¤‡ç‰¹å¾æå–å™¨ã€åˆ†è¯å™¨å’Œæ•°æ®](#å‡†å¤‡ç‰¹å¾æå–å™¨åˆ†è¯å™¨å’Œæ•°æ®)
    4. [è®­ç»ƒä¸è¯„ä¼°](#è®­ç»ƒä¸è¯„ä¼°)
    5. [æ„å»ºæ¼”ç¤ºåº”ç”¨](#æ„å»ºæ¼”ç¤ºåº”ç”¨)
3. [ç»“æŸè¯­](#ç»“æŸè¯­)

## ç®€ä»‹

Whisper æ˜¯æ¥è‡ªäº OpenAI çš„ç”± Alec Radford ç­‰äººäº [2022 å¹´ 9 æœˆ](https://openai.com/blog/whisper/) å‘è¡¨çš„ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ« (automatic speech recognitionï¼ŒASR) çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸ [Wav2Vec 2.0](https://arxiv.org/abs/2006.11477) ç­‰å‰ä½œä¸åŒï¼Œä¹‹å‰çš„æ¨¡å‹æ˜¯åœ¨æœªæ ‡è®°çš„éŸ³é¢‘æ•°æ®ä¸Šé¢„è®­ç»ƒçš„ï¼Œè€Œ Whisper æ˜¯åœ¨å¤§é‡çš„**æ ‡æ³¨**éŸ³é¢‘è½¬å½•æ•°æ®ä¸Šé¢„è®­ç»ƒçš„ã€‚å…¶è®­ç»ƒæ ‡æ³¨è®­ç»ƒæ•°æ®é«˜è¾¾ 68 ä¸‡å°æ—¶ï¼Œæ¯”ç”¨äºè®­ç»ƒ Wav2Vec 2.0 çš„æœªæ ‡æ³¨éŸ³é¢‘æ•°æ®ï¼ˆ6 ä¸‡å°æ—¶ï¼‰è¿˜å¤šä¸€ä¸ªæ•°é‡çº§ã€‚æ›´æœ‰ç”šè€…ï¼Œè¯¥é¢„è®­ç»ƒæ•°æ®ä¸­æœ‰ 11.7 ä¸‡å°æ—¶å¤šè¯­ç§è¯­éŸ³è¯†åˆ«æ•°æ®ã€‚å› æ­¤ï¼ŒWhisper è®­å¾—çš„ checkpoint å¯åº”ç”¨äºè¶…è¿‡ 96 ç§è¯­è¨€ï¼Œå…¶ä¸­è®¸å¤šè¯­è¨€*ç¼ºä¹è®­ç»ƒæ•°æ®*ã€‚

è¿™ä¹ˆå¤šçš„æ ‡æ³¨æ•°æ®ä½¿å¾—æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨*æœ‰ç›‘ç£*è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šé¢„è®­ç»ƒ Whisperï¼Œä»è€Œä»æ ‡æ³¨éŸ³é¢‘è½¬å½•é¢„è®­ç»ƒæ•°æ® ${}^1$ ä¸­ä¹ å¾—è¯­éŸ³åˆ°æ–‡æœ¬çš„æ˜ å°„ã€‚å› æ­¤ï¼ŒWhisper å‡ ä¹ä¸éœ€è¦é¢å¤–çš„å¾®è°ƒå°±å·²ç»æ˜¯é«˜æ€§èƒ½çš„ ASR æ¨¡å‹äº†ã€‚è¿™ä½¿å¾— Wav2Vec 2.0 ç›¸å½¢è§ç»Œï¼ŒWav2Vec 2.0 æ˜¯åœ¨*æ— ç›‘ç£çš„*æ©ç é¢„æµ‹ä»»åŠ¡ä¸Šé¢„è®­ç»ƒçš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹ç»è¿‡è®­ç»ƒå¯ä»¥ä»æœªæ ‡æ³¨çš„çº¯éŸ³é¢‘æ•°æ®ä¸­ä¹ å¾—ä»è¯­éŸ³åˆ°éšå«çŠ¶æ€çš„ä¸­é—´æ˜ å°„ã€‚è™½ç„¶æ— ç›‘ç£é¢„è®­ç»ƒäº§ç”Ÿäº†é«˜è´¨é‡çš„è¯­éŸ³è¡¨ç¤ºï¼Œä½†å®ƒ**å­¦ä¸åˆ°**è¯­éŸ³åˆ°æ–‡æœ¬çš„æ˜ å°„ï¼Œè¯¥æ˜ å°„åªèƒ½åœ¨å¾®è°ƒæœŸé—´å­¦ä¹ ã€‚å› æ­¤ï¼Œå…¶éœ€è¦æ›´å¤šçš„å¾®è°ƒæ‰èƒ½ä¿è¯å…¶æ€§èƒ½æœ‰ç«äº‰åŠ›ã€‚

å½“æ‰©å±•åˆ° 68 ä¸‡å°æ—¶çš„æ ‡æ³¨é¢„è®­ç»ƒæ•°æ®æ—¶ï¼ŒWhisper æ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–åˆ°å¤šæ•°æ®é›†å’Œé¢†åŸŸçš„èƒ½åŠ›ã€‚å…¶é¢„è®­ç»ƒ checkpoint è¡¨ç°å‡ºäº†ä¸æœ€å…ˆè¿›çš„ ASR ç³»ç»Ÿæ——é¼“ç›¸å½“çš„ç»“æœï¼šåœ¨LibriSpeech ASR çš„æ— å™ªæµ‹è¯•å­é›†ä¸Šè¾¾åˆ°äº†æ¥è¿‘ 3% çš„å•è¯é”™è¯¯ç‡ï¼ˆword error rateï¼ŒWERï¼‰ï¼Œåœ¨ TED-LIUM ä¸Šåˆ›ä¸‹äº†æ–°çš„æœ€ä½³è®°å½• - 4.7% çš„ WERï¼ˆ*è¯¦è§* [Whisper è®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf)çš„è¡¨ 8ï¼‰ã€‚Whisper åœ¨é¢„è®­ç»ƒæœŸé—´è·å¾—çš„å¹¿æ³›çš„å¤šè¯­ç§ ASR çŸ¥è¯†å¯¹ä¸€äº›ç¼ºä¹æ•°æ®çš„è¯­ç§ç‰¹åˆ«æœ‰ç”¨ï¼›é€šè¿‡å¾®è°ƒï¼Œé¢„è®­ç»ƒçš„ checkpoint å¯ä»¥è¿›ä¸€æ­¥é€‚é…ç‰¹å®šçš„æ•°æ®é›†å’Œè¯­è¨€ï¼Œä»è€Œè¿›ä¸€æ­¥æ”¹è¿›åœ¨è¿™äº›è¯­è¨€ä¸Šçš„è¯†åˆ«æ•ˆæœã€‚

Whisper æ˜¯ä¸€ä¸ªåŸºäº Transformer çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼ˆä¹Ÿç§°ä¸º*åºåˆ—åˆ°åºåˆ—*æ¨¡å‹ï¼‰ã€‚å®ƒå°†éŸ³é¢‘é¢‘è°±å›¾ç‰¹å¾*åºåˆ—*æ˜ å°„åˆ°æ–‡æœ¬è¯*åºåˆ—*ã€‚é¦–å…ˆï¼Œé€šè¿‡ç‰¹å¾æå–å™¨çš„æ“ä½œå°†åŸå§‹éŸ³é¢‘è¾“å…¥è½¬æ¢ä¸ºå¯¹æ•°æ¢…å°”å£°è°±å›¾ï¼ˆlog-Mel spectrogramï¼‰ã€‚ç„¶åï¼ŒTransformer ç¼–ç å™¨å¯¹é¢‘è°±å›¾è¿›è¡Œç¼–ç ï¼Œå½¢æˆä¸€ç³»åˆ—ç¼–ç å™¨éšå«çŠ¶æ€ã€‚æœ€åï¼Œè§£ç å™¨ä»¥å…ˆå‰çš„è¯ä»¥åŠç¼–ç å™¨çš„éšå«çŠ¶æ€ä¸ºæ¡ä»¶ï¼Œè‡ªå›å½’åœ°é¢„æµ‹æ–‡æœ¬çš„ä¸‹ä¸€ä¸ªè¯ã€‚å›¾ 1 æ˜¯ Whisper æ¨¡å‹çš„ç¤ºæ„å›¾ã€‚

<figure>
<img src="assets/111_fine_tune_whisper/whisper_architecture.svg" alt="Trulli" style="width:100%">
<figcaption align = "center"><b>å›¾ 1:</b> Whisper æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å±äºæ ‡å‡†çš„åŸºäº Transformer çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚ é¦–å…ˆå°†å¯¹æ•°æ¢…å°”å£°è°±å›¾è¾“å…¥åˆ°ç¼–ç å™¨ï¼Œç¼–ç å™¨ç”Ÿæˆçš„æœ€ç»ˆéšå«çŠ¶æ€ä¼šé€šè¿‡äº¤å‰æ³¨æ„æœºåˆ¶è¾“å…¥ç»™è§£ç å™¨ã€‚è§£ç å™¨å†åŸºäºç¼–ç å™¨éšå«çŠ¶æ€å’Œå…ˆå‰çš„é¢„æµ‹è¯ï¼Œè‡ªå›å½’åœ°é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºè¯ã€‚å›¾æº: <a href="https://openai.com/blog/whisper/">OpenAI Whisper åšå®¢</a>ã€‚</figcaption>
</figure>

åœ¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­ï¼Œç¼–ç å™¨è´Ÿè´£ä»è¯­éŸ³ä¸­æå–é‡è¦ç‰¹å¾ï¼Œå°†éŸ³é¢‘è¾“å…¥è½¬æ¢ä¸ºä¸€ç»„éšå«çŠ¶æ€è¡¨å¾ã€‚è§£ç å™¨æ‰®æ¼”è¯­è¨€æ¨¡å‹çš„è§’è‰²ï¼Œå¤„ç†éšå«çŠ¶æ€è¡¨å¾å¹¶ç”Ÿæˆç›¸åº”çš„æ–‡ç¨¿ã€‚åœ¨æ¨¡å‹æ¶æ„**å†…éƒ¨**é›†æˆè¯­è¨€æ¨¡å‹çš„è¡Œä¸ºæˆ‘ä»¬ç§°ä¹‹ä¸º*æ·±åº¦èåˆ*ã€‚ä¸ä¹‹ç›¸å¯¹çš„æ˜¯*æµ…èåˆ*ï¼Œæ­¤æ—¶ï¼Œè¯­è¨€æ¨¡å‹æ˜¯åœ¨**å¤–éƒ¨**ä¸ç¼–ç å™¨ç»„åˆçš„ï¼Œå¦‚ CTC + $n$-gram (*è¯¦è§* [Internal Language Model Estimation](https://arxiv.org/pdf/2011.01991.pdf) ä¸€æ–‡ï¼‰ã€‚é€šè¿‡æ·±åº¦èåˆï¼Œå¯ä»¥ç”¨ç›¸åŒçš„è®­ç»ƒæ•°æ®å’ŒæŸå¤±å‡½æ•°å¯¹æ•´ä¸ªç³»ç»Ÿè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œä»è€Œæä¾›æ›´å¤§çš„çµæ´»æ€§å’Œæ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼ˆ*è¯¦è§* [ESB Benchmark](https://arxiv.org/abs/2210.13352)ï¼‰ã€‚

Whisper ä½¿ç”¨äº¤å‰ç†µç›®æ ‡å‡½æ•°è¿›è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œäº¤å‰ç†µç›®æ ‡å‡½æ•°æ˜¯è®­ç»ƒåºåˆ—åˆ°åºåˆ—çš„åˆ†ç±»æ¨¡å‹çš„æ ‡å‡†ç›®æ ‡å‡½æ•°ã€‚è¿™é‡Œï¼Œç³»ç»Ÿç»è¿‡è®­ç»ƒå¯ä»¥ä»é¢„å®šä¹‰çš„è¯æ±‡è¡¨ä¸­æ­£ç¡®åœ°å¯¹ç›®æ ‡è¯è¿›è¡Œåˆ†ç±»ï¼Œä»è€Œäº§ç”Ÿè¾“å‡ºè¯ã€‚

Whisper æœ‰äº”ç§ä¸åŒå°ºå¯¸çš„ checkpointsã€‚æœ€å°çš„å››ä¸ªå°ºå¯¸åˆ†åˆ«æœ‰åŸºäºçº¯è‹±è¯­è®­ç»ƒçš„ç‰ˆæœ¬å’ŒåŸºäºå¤šè¯­ç§æ•°æ®è®­ç»ƒçš„ç‰ˆæœ¬ã€‚æœ€å¤§çš„ checkpoint åªæœ‰åŸºäºå¤šè¯­ç§æ•°æ®è®­ç»ƒçš„ç‰ˆæœ¬ã€‚[Hugging Face Hub](https://huggingface.co/models?search=openai/whisper) ä¸Šæä¾›äº†æ‰€æœ‰ä¹ä¸ªé¢„è®­ç»ƒ checkpointsã€‚ä¸‹è¡¨æ€»ç»“äº†è¿™äº› checkpoints çš„ä¿¡æ¯ï¼Œå¹¶äº†å„è‡ªçš„ Hub é“¾æ¥ï¼š

| å°ºå¯¸   | å±‚æ•° | å®½ | å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•° | å‚æ•°é‡ | çº¯è‹±è¯­ checkpoint                                         | å¤šè¯­ç§ checkpoint                                      |
|--------|--------|-------|-------|------------|------------------------------------------------------|---------------------------------------------------|
| tiny   | 4      | 384   | 6     | 39 M       | [âœ“](https://huggingface.co/openai/whisper-tiny.en)   | [âœ“](https://huggingface.co/openai/whisper-tiny.)  |
| base   | 6      | 512   | 8     | 74 M       | [âœ“](https://huggingface.co/openai/whisper-base.en)   | [âœ“](https://huggingface.co/openai/whisper-base)   |
| small  | 12     | 768   | 12    | 244 M      | [âœ“](https://huggingface.co/openai/whisper-small.en)  | [âœ“](https://huggingface.co/openai/whisper-small)  |
| medium | 24     | 1024  | 16    | 769 M      | [âœ“](https://huggingface.co/openai/whisper-medium.en) | [âœ“](https://huggingface.co/openai/whisper-medium) |
| large  | 32     | 1280  | 20    | 1550 M     | x                                                    | [âœ“](https://huggingface.co/openai/whisper-large)  |

ä¸‹æ–‡æˆ‘ä»¬å°†ä»¥å¾®è°ƒå¤šè¯­ç§ç‰ˆçš„ [`small`](https://huggingface.co/openai/whisper-small)ï¼ˆå‚æ•°é‡ 244M (~= 1GB)ï¼‰checkpoint ä¸ºä¾‹ï¼Œå¸¦å¤§å®¶èµ°ä¸€éå…¨ç¨‹ã€‚æ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0) æ•°æ®é›†é‡Œçš„å°è¯­ç§æ•°æ®æ¥è®­ç»ƒå’Œè¯„ä¼°æˆ‘ä»¬çš„ç³»ç»Ÿã€‚æˆ‘ä»¬å°†è¯æ˜ï¼Œåªéœ€ 8 å°æ—¶çš„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾®è°ƒå‡ºä¸€ä¸ªåœ¨è¯¥è¯­ç§ä¸Šè¡¨ç°å¼ºå¤§çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚

------------------------------------------------------------------------

${}^1$ Whisper çš„åç§°æ¥è‡ªäº â€œWeb-scale Supervised Pre-training for Speech Recognitionï¼ˆç½‘ç»œè§„æ¨¡çš„æœ‰ç›‘ç£è¯­éŸ³è¯†åˆ«é¢„è®­ç»ƒæ¨¡å‹ï¼‰â€ çš„é¦–å­—æ¯ç¼©å†™ â€œWSPSRâ€ã€‚

## åœ¨ Google Colab ä¸­å¾®è°ƒ Whisper

### å‡†å¤‡ç¯å¢ƒ

æˆ‘ä»¬å°†ä½¿ç”¨å‡ ä¸ªæµè¡Œçš„ Python åŒ…æ¥å¾®è°ƒ Whisper æ¨¡å‹ã€‚æˆ‘ä»¬ä¼šä½¿ç”¨ `datasets` æ¥ä¸‹è½½å’Œå‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨`transformers` æ¥åŠ è½½å’Œè®­ç»ƒ Whisper æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜éœ€è¦ `soundfile` åŒ…æ¥é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼Œ`evaluate` å’Œ `jiwer` æ¥è¯„ä¼°æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨ `gradio` ä¸ºæˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹æ„å»ºä¸€ä¸ªäº®é—ªé—ªçš„æ¼”ç¤ºã€‚

```bash
!pip install datasets>=2.6.1
!pip install git+https://github.com/huggingface/transformers
!pip install librosa
!pip install evaluate>=0.30
!pip install jiwer
!pip install gradio
```

æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½ åœ¨è®­ç»ƒæ—¶ç›´æ¥å°†æ¨¡å‹ checkpoint ä¸Šä¼ åˆ° [Hugging Face Hub](https://huggingface.co/)ã€‚å®ƒæä¾›äº†ä»¥ä¸‹åŠŸèƒ½ï¼š
- é›†æˆç‰ˆæœ¬æ§åˆ¶ï¼šç¡®ä¿åœ¨è®­ç»ƒæœŸé—´ä¸ä¼šä¸¢å¤±ä»»ä½•æ¨¡å‹ checkpointã€‚
- Tensorboard æ—¥å¿—ï¼šè·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„é‡è¦æŒ‡æ ‡ã€‚
- æ¨¡å‹å¡ï¼šè®°å½•æ¨¡å‹çš„ä½œç”¨åŠå…¶åº”ç”¨åœºæ™¯ã€‚
- ç¤¾åŒºï¼šè½»æ¾ä¸ç¤¾åŒºè¿›è¡Œåˆ†äº«å’Œåä½œï¼

å°† Python notebook é“¾æ¥åˆ° Hub éå¸¸ç®€å• - åªéœ€åœ¨å‡ºç°æç¤ºæ—¶è¾“å…¥ä½ çš„ Hub èº«ä»½éªŒè¯ä»¤ç‰Œå³å¯ã€‚ä½ å¯ä»¥åœ¨[æ­¤å¤„](https://huggingface.co/settings/tokens)æ‰¾åˆ°ä½ çš„ Hub èº«ä»½éªŒè¯ä»¤ç‰Œï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

**æ‰“å°è¾“å‡ºï¼š**
```bash
Login successful
Your token has been saved to /root/.huggingface/token
```

### åŠ è½½æ•°æ®é›†

Common Voice ç”±ä¸€ç³»åˆ—ä¼—åŒ…æ•°æ®é›†ç»„æˆï¼Œå…¶ä¸­åŒ…å«äº†ç”¨å„ç§è¯­è¨€å½•åˆ¶çš„æ¥è‡ªç»´åŸºç™¾ç§‘çš„æ–‡æœ¬ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ Common Voice æ•°æ®é›†ï¼ˆ[ç‰ˆæœ¬ 11](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)ï¼‰ã€‚è¯­ç§ä¸Šï¼Œæˆ‘ä»¬é€‰æ‹©äº†ç”¨[*å°åœ°è¯­*](https://en.wikipedia.org/wiki/Hindi) æ¥å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚å°åœ°è¯­æ˜¯ä¸€ç§åœ¨å°åº¦åŒ—éƒ¨ã€ä¸­éƒ¨ã€ä¸œéƒ¨å’Œè¥¿éƒ¨ä½¿ç”¨çš„å°åº¦-é›…åˆ©å®‰è¯­ã€‚Common Voice 11.0 åŒ…å«å¤§çº¦ 12 å°æ—¶çš„æ ‡æ³¨å°åœ°è¯­æ•°æ®ï¼Œå…¶ä¸­ 4 å°æ—¶æ˜¯æµ‹è¯•æ•°æ®ã€‚

æˆ‘ä»¬å…ˆå‰å¾€ Hub æŸ¥çœ‹ Common Voice çš„æ•°æ®é›†é¡µé¢ï¼š[mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)ã€‚

å½“é¦–æ¬¡æŸ¥çœ‹æ­¤é¡µé¢æ—¶ï¼Œç³»ç»Ÿä¼šè¦æ±‚æˆ‘ä»¬æ¥å—å…¶ä½¿ç”¨æ¡æ¬¾ã€‚ä¹‹åï¼Œæˆ‘ä»¬å³å¯è·å¾—å¯¹æ•°æ®é›†çš„å®Œå…¨è®¿é—®æƒé™ã€‚

ä¸€æ—¦éªŒè¯äº†èº«ä»½ï¼Œæˆ‘ä»¬å°±ä¼šçœ‹åˆ°æ•°æ®é›†é¢„è§ˆã€‚æ•°æ®é›†é¢„è§ˆå±•ç¤ºäº†æ•°æ®é›†çš„å‰ 100 ä¸ªæ ·æœ¬ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒåŠ è½½äº†å¯ä¾›æˆ‘ä»¬å®æ—¶æ”¶å¬çš„éŸ³é¢‘æ ·æœ¬ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ä¸‹æ‹‰èœå•å°†å­é›†è®¾ç½®ä¸º `hi` æ¥é€‰æ‹© Common Voice çš„å°åœ°è¯­å­é›†ï¼ˆ`hi` æ˜¯å°åœ°è¯­çš„è¯­è¨€æ ‡è¯†ç¬¦ä»£ç ï¼‰ï¼š

<figure>
<img src="assets/111_fine_tune_whisper/select_hi.jpg" alt="Trulli" style="width:100%">
</figure>

å¦‚æœç‚¹å‡»ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ’­æ”¾æŒ‰é’®ï¼Œæˆ‘ä»¬å¯ä»¥æ”¶å¬éŸ³é¢‘å¹¶çœ‹åˆ°ç›¸åº”çš„æ–‡æœ¬ã€‚æˆ‘ä»¬è¿˜å¯ä»¥æ»šåŠ¨æµè§ˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ ·æœ¬ï¼Œä»¥æ›´å¥½åœ°äº†è§£å¾…å¤„ç†çš„éŸ³é¢‘å’Œæ–‡æœ¬æ•°æ®ã€‚ä»è¯­è°ƒå’Œé£æ ¼å¯ä»¥çœ‹å‡ºè¿™äº›å½•éŸ³æ¥è‡ªäºæ—ç™½ã€‚ä½ å¯èƒ½è¿˜ä¼šæ³¨æ„åˆ°å½•éŸ³è€…å’Œå½•éŸ³è´¨é‡çš„å·¨å¤§å·®å¼‚ï¼Œè¿™æ˜¯ä¼—åŒ…æ•°æ®çš„ä¸€ä¸ªå…±åŒç‰¹å¾ã€‚

ä½¿ç”¨ ğŸ¤— Datasetsï¼Œä¸‹è½½å’Œå‡†å¤‡æ•°æ®éå¸¸ç®€å•ã€‚æˆ‘ä»¬åªéœ€ä¸€è¡Œä»£ç å³å¯ä¸‹è½½å’Œå‡†å¤‡ Common Voice æ•°æ®é›†ã€‚ç”±äºå°åœ°è¯­æ•°æ®éå¸¸åŒ®ä¹ï¼Œæˆ‘ä»¬æŠŠ`è®­ç»ƒé›†`å’Œ`éªŒè¯é›†`åˆå¹¶æˆçº¦ 8 å°æ—¶çš„æ•°æ®ç”¨äºè®­ç»ƒï¼Œè€Œæµ‹è¯•åˆ™åŸºäº 4 å°æ—¶çš„`æµ‹è¯•é›†`ï¼š

```python
from datasets import load_dataset, DatasetDict

common_voice = DatasetDict()

common_voice["train"] = load_dataset("mozilla-foundation/common_voice_11_0", "hi", split="train+validation", use_auth_token=True)
common_voice["test"] = load_dataset("mozilla-foundation/common_voice_11_0", "hi", split="test", use_auth_token=True)

print(common_voice)
```

**æ‰“å°è¾“å‡ºï¼š**
```
DatasetDict({
    train: Dataset({
        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],
        num_rows: 6540
    })
    test: Dataset({
        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],
        num_rows: 2894
    })
})
```

å¤§å¤šæ•° ASR æ•°æ®é›†ä»…æä¾›è¾“å…¥éŸ³é¢‘æ ·æœ¬ (`audio`) å’Œç›¸åº”çš„è½¬å½•æ–‡æœ¬ (`sentence`)ã€‚ Common Voice åŒ…å«é¢å¤–çš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œä¾‹å¦‚ `accent` å’Œ `locale`ï¼Œå¯¹ ASR åœºæ™¯ï¼Œæˆ‘ä»¬å¯ä»¥å¿½ç•¥è¿™äº›æ•°æ®ã€‚ä¸ºäº†ä½¿ä»£ç å°½å¯èƒ½é€šç”¨ï¼Œæˆ‘ä»¬åªè€ƒè™‘åŸºäºè¾“å…¥éŸ³é¢‘å’Œè½¬å½•æ–‡æœ¬è¿›è¡Œå¾®è°ƒï¼Œè€Œä¸¢å¼ƒæ‰é¢å¤–çš„å…ƒæ•°æ®ä¿¡æ¯ï¼š

```python
common_voice = common_voice.remove_columns(["accent", "age", "client_id", "down_votes", "gender", "locale", "path", "segment", "up_votes"])
```

Common Voice åªæ˜¯æˆ‘ä»¬å¯ä» Hub ä¸Šä¸‹è½½çš„ä¼—å¤šçš„å¤šè¯­ç§ ASR æ•°æ®é›†ä¸­çš„ä¸€ä¸ª â€”â€” Hub ä¸Šè¿˜æœ‰å¾ˆå¤šå¯ä¾›æˆ‘ä»¬ä½¿ç”¨ï¼æƒ³çŸ¥é“ Hub ä¸Šæœ‰å“ªäº›å¯ç”¨äºè¯­éŸ³è¯†åˆ«çš„æ•°æ®é›†ï¼Œè¯·ç‚¹å‡»é“¾æ¥ï¼š[Hub ä¸Šçš„ ASR æ•°æ®é›†](https://huggingface.co/datasets?task_categories=task_categories:automatic-speech-recognition&sort=downloads)ã€‚

### å‡†å¤‡ç‰¹å¾æå–å™¨ã€åˆ†è¯å™¨å’Œæ•°æ®

ASR çš„æµæ°´çº¿ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼š
1ï¼‰é¢„å¤„ç†åŸå§‹éŸ³é¢‘è¾“å…¥çš„ç‰¹å¾æå–å™¨
2ï¼‰æ‰§è¡Œåºåˆ—åˆ°åºåˆ—æ˜ å°„çš„æ¨¡å‹
3) å°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºæ–‡æœ¬çš„åˆ†è¯å™¨

åœ¨ ğŸ¤— Transformers ä¸­ï¼ŒWhisper æ¨¡å‹æœ‰ä¸€ä¸ªå…³è”çš„ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ï¼Œå³ [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor) å’Œ [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)ã€‚

ä¸‹é¢ï¼Œæˆ‘ä»¬é€ä¸€è¯¦ç»†ä»‹ç»ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ï¼

### åŠ è½½ WhisperFeatureExtractor

è¯­éŸ³å¯è¡¨ç¤ºä¸ºéšæ—¶é—´å˜åŒ–çš„ä¸€ç»´æ•°ç»„ï¼Œç»™å®šæ—¶åˆ»çš„æ•°ç»„å€¼è¡¨ç¤ºä¿¡å·åœ¨è¯¥æ—¶åˆ»çš„*å¹…åº¦*ï¼Œè€Œæˆ‘ä»¬å¯ä»¥ä»…ä»å¹…åº¦ä¿¡æ¯é‡å»ºéŸ³é¢‘çš„é¢‘è°±å¹¶æ¢å¤æ‰€æœ‰å£°å­¦ç‰¹å¾ã€‚

ç”±äºè¯­éŸ³æ˜¯è¿ç»­çš„ï¼Œå› æ­¤å®ƒåŒ…å«æ— æ•°ä¸ªå¹…åº¦å€¼ï¼Œè€Œè®¡ç®—æœºåªèƒ½è¡¨ç¤ºå¹¶å­˜å‚¨æœ‰é™ä¸ªå€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡å¯¹è¯­éŸ³ä¿¡å·è¿›è¡Œç¦»æ•£åŒ–ï¼Œå³ä»¥å›ºå®šæ—¶é—´çš„é—´éš”å¯¹è¿ç»­ä¿¡å·è¿›è¡Œ*é‡‡æ ·*ã€‚æˆ‘ä»¬å°†æ¯ç§’é‡‡æ ·çš„æ¬¡æ•°ç§°ä¸º*é‡‡æ ·ç‡*ï¼Œé€šå¸¸ä»¥æ ·æœ¬/ç§’æˆ–*èµ«å…¹ (Hz)* ä¸ºå•ä½ã€‚é«˜é‡‡æ ·ç‡å¯ä»¥æ›´å¥½åœ°è¿‘ä¼¼è¿ç»­è¯­éŸ³ä¿¡å·ï¼Œä½†åŒæ—¶æ¯ç§’éœ€è¦å­˜å‚¨çš„å€¼ä¹Ÿæ›´å¤šã€‚

éœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼Œè¾“å…¥éŸ³é¢‘çš„é‡‡æ ·ç‡éœ€è¦ä¸æ¨¡å‹æœŸæœ›çš„é‡‡æ ·ç‡ç›¸åŒ¹é…ï¼Œå› ä¸ºä¸åŒé‡‡æ ·ç‡çš„éŸ³é¢‘ä¿¡å·ä¹‹é—´çš„åˆ†å¸ƒæ˜¯ä¸åŒçš„ã€‚å¤„ç†éŸ³é¢‘æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ­£ç¡®çš„é‡‡æ ·ç‡ï¼Œå¦åˆ™å¯èƒ½ä¼šå¼•èµ·æ„æƒ³ä¸åˆ°çš„ç»“æœï¼ä¾‹å¦‚ï¼Œä»¥ 16kHz çš„é‡‡æ ·ç‡é‡‡é›†éŸ³é¢‘ä½†ä»¥ 8kHz çš„é‡‡æ ·ç‡æ”¶å¬å®ƒï¼Œä¼šä½¿éŸ³é¢‘å¬èµ·æ¥å¥½åƒæ˜¯åŠé€Ÿçš„ã€‚åŒæ ·åœ°ï¼Œå‘ä¸€ä¸ªéœ€è¦æŸä¸€é‡‡æ ·ç‡çš„ ASR æ¨¡å‹é¦ˆé€ä¸€ä¸ªé”™è¯¯é‡‡æ ·ç‡çš„éŸ³é¢‘ä¹Ÿä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚ Whisper ç‰¹å¾æå–å™¨éœ€è¦é‡‡æ ·ç‡ä¸º 16kHz çš„éŸ³é¢‘è¾“å…¥ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†è¾“å…¥çš„é‡‡æ ·ç‡ä¸ä¹‹ç›¸åŒ¹é…ã€‚æˆ‘ä»¬ä¸æƒ³æ— æ„ä¸­ç”¨æ…¢é€Ÿè¯­éŸ³æ¥è®­ç»ƒ ASRï¼

Whisper ç‰¹å¾æå–å™¨æ‰§è¡Œä¸¤ä¸ªæ“ä½œã€‚é¦–å…ˆï¼Œå¡«å……/æˆªæ–­ä¸€æ‰¹éŸ³é¢‘æ ·æœ¬ï¼Œä½¿æ‰€æœ‰æ ·æœ¬çš„è¾“å…¥é•¿åº¦å‡ä¸º 30 ç§’ã€‚é€šè¿‡åœ¨åºåˆ—æœ«å°¾é™„åŠ é›¶ï¼ˆéŸ³é¢‘ä¿¡å·ä¸­çš„é›¶å¯¹åº”äºæ— ä¿¡å·æˆ–é™éŸ³ï¼‰ï¼Œå°†çŸ­äº 30 ç§’çš„æ ·æœ¬å¡«å……åˆ° 30 ç§’ã€‚å¹¶å°†è¶…è¿‡ 30 ç§’çš„æ ·æœ¬è¢«æˆªæ–­ä¸º 30 ç§’ã€‚ç”±äºè¿™ä¸€æ‰¹æ•°æ®ä¸­çš„æ‰€æœ‰å…ƒç´ éƒ½è¢«å¡«å……/æˆªæ–­åˆ°æœ€å¤§é•¿åº¦ï¼ˆå³ 30 sï¼‰ï¼Œå› æ­¤åœ¨å°†éŸ³é¢‘é¦ˆé€ç»™ Whisper æ¨¡å‹æ—¶æˆ‘ä»¬å°±ä¸éœ€è¦æ³¨æ„åŠ›æ©ç äº†ã€‚ Whisper åœ¨è¿™æ–¹é¢æ˜¯ç‹¬ä¸€æ— äºŒçš„ â€”â€” å¯¹å¤§å¤šæ•°éŸ³é¢‘æ¨¡å‹è€Œè¨€ï¼Œä½ éœ€è¦æä¾›ä¸€ä¸ªæ³¨æ„åŠ›æ©ç ï¼Œè¯¦ç»†è¯´æ˜å¡«å……åºåˆ—çš„ä½ç½®ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­å¿½ç•¥å¡«å……åºåˆ—ã€‚ Whisper ç»è¿‡è®­ç»ƒå¯ä»¥åœ¨æ²¡æœ‰æ³¨æ„åŠ›æ©ç çš„æƒ…å†µä¸‹è¿è¡Œï¼Œå®ƒå¯ä»¥ç›´æ¥ä»è¯­éŸ³ä¿¡å·ä¸­æ¨æ–­å‡ºåº”è¯¥å¿½ç•¥å“ªäº›ã€‚

Whisper ç‰¹å¾æå–å™¨æ‰§è¡Œçš„ç¬¬äºŒä¸ªæ“ä½œæ˜¯å°†å¡«å……çš„éŸ³é¢‘æ•°ç»„å˜æ¢ä¸ºå¯¹æ•°æ¢…å°”å£°è°±å›¾ã€‚è¿™äº›é¢‘è°±å›¾æ˜¯ä¿¡å·é¢‘ç‡çš„ç›´è§‚è¡¨ç¤ºï¼Œå¾ˆåƒå‚…é‡Œå¶å˜æ¢ã€‚å›¾ 2 æ˜¾ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹é¢‘è°±å›¾ã€‚$y$ è½´è¡¨ç¤ºæ¢…å°”é¢‘æ®µï¼ˆMel channelï¼‰ï¼Œå®ƒä»¬å¯¹åº”äºç‰¹å®šçš„é¢‘æ®µã€‚$x$ è½´æ˜¯æ—¶é—´ã€‚æ¯ä¸ªåƒç´ çš„é¢œè‰²å¯¹åº”äºç»™å®šæ—¶é—´è¯¥é¢‘æ®µçš„å¯¹æ•°å¼ºåº¦ã€‚å¯¹æ•°æ¢…å°”å£°è°±å›¾æ˜¯ Whisper æ¨¡å‹éœ€è¦çš„è¾“å…¥å½¢å¼ã€‚

æ¢…å°”é¢‘æ®µæ˜¯è¯­éŸ³å¤„ç†ä¸­çš„æ ‡å‡†è¡¨ç¤ºå½¢å¼ï¼Œç ”ç©¶äººå‘˜é€‰æ‹©ç”¨å®ƒæ¥è¿‘ä¼¼äººç±»çš„å¬è§‰èŒƒå›´ã€‚å¯¹äº Whisper å¾®è°ƒè¿™ä¸ªä»»åŠ¡è€Œè¨€ï¼Œæˆ‘ä»¬åªéœ€è¦çŸ¥é“å£°è°±å›¾æ˜¯è¯­éŸ³ä¿¡å·ä¸­é¢‘ç‡çš„ç›´è§‚è¡¨ç¤ºã€‚æœ‰å…³æ¢…å°”é¢‘æ®µçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ¢…å°”é¢‘ç‡å€’è°±](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)è¿™ç¯‡æ–‡ç« ã€‚

<figure>
<img src="assets/111_fine_tune_whisper/spectrogram.jpg" alt="Trulli" style="width:100%">
<figcaption align = "center"><b>å›¾ 2ï¼š</b> é‡‡æ ·çš„éŸ³é¢‘ä¿¡å·åˆ°å¯¹æ•°æ¢…å°”å£°è°±å›¾çš„è½¬æ¢ã€‚å·¦ï¼šé‡‡æ ·çš„ä¸€ç»´éŸ³é¢‘ä¿¡å·ã€‚å³å›¾ï¼šç›¸åº”çš„å¯¹æ•°æ¢…å°”å£°è°±å›¾ã€‚å›¾æºï¼š<a href="https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html">è°·æ­Œ SpecAugment åšæ–‡</a>. </figcaption>
</figure>

å¹¸è¿çš„æ˜¯ï¼ŒğŸ¤— Transformers Whisper ç‰¹å¾æå–å™¨ä»…ç”¨ä¸€è¡Œä»£ç å³å¯æ‰§è¡Œå¡«å……å’Œå£°è°±å›¾å˜æ¢ï¼è®©æˆ‘ä»¬ç»§ç»­ä»é¢„è®­ç»ƒçš„ checkpoint ä¸­åŠ è½½ç‰¹å¾æå–å™¨ï¼Œä¸ºéŸ³é¢‘æ•°æ®å¤„ç†åšå¥½å‡†å¤‡ï¼š

```python
from transformers import WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")
```

### åŠ è½½ WhisperTokenizer

ç°åœ¨æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åŠ è½½ Whisper åˆ†è¯å™¨ã€‚ Whisper æ¨¡å‹ä¼šè¾“å‡ºè¯å…ƒï¼Œè¿™äº›è¯å…ƒè¡¨ç¤ºé¢„æµ‹æ–‡æœ¬åœ¨è¯å…¸ä¸­çš„ç´¢å¼•ã€‚åˆ†è¯å™¨è´Ÿè´£å°†è¿™ä¸€ç³»åˆ—è¯å…ƒæ˜ å°„åˆ°å®é™…çš„æ–‡æœ¬å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ [1169, 3797, 3332] -> "the cat sat"ï¼‰ã€‚

ä¼ ç»Ÿä¸Šï¼Œå½“ä½¿ç”¨ç¼–ç å™¨æ¨¡å‹è¿›è¡Œ ASR æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ [_Connectionist Temporal Classification (CTC)_](https://distill.pub/2017/ctc/) è¿›è¡Œè§£ç ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ¯ä¸ªæ•°æ®é›†è®­ç»ƒä¸€ä¸ª CTC åˆ†è¯å™¨ã€‚ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„ä¼˜åŠ¿ä¹‹ä¸€æ˜¯æˆ‘ä»¬å¯ä»¥ç›´æ¥æ˜¯ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†è¯å™¨ã€‚

Whisper åˆ†è¯å™¨é’ˆå¯¹ 96 ç§é¢„è®­ç»ƒè¯­è¨€è¿›è¡Œäº†é¢„è®­ç»ƒã€‚å› æ­¤ï¼Œå®ƒçš„[å­—èŠ‚å¯¹ï¼ˆbyte-pairï¼‰](https://huggingface.co/course/chapter6/5?fw=pt#bytepair-encoding-tokenization)æ¶µç›–èŒƒå›´å¾ˆå¹¿ï¼Œå‡ ä¹é€‚ç”¨äºæ‰€æœ‰å¤šè¯­ç§ ASR åº”ç”¨ã€‚å¯¹äºå°åœ°è¯­ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½åˆ†è¯å™¨å¹¶å°†å…¶ç”¨äºå¾®è°ƒè€Œæ— éœ€ä»»ä½•è¿›ä¸€æ­¥ä¿®æ”¹ã€‚æˆ‘ä»¬åªéœ€æŒ‡å®šç›®æ ‡è¯­è¨€å’Œä»»åŠ¡ï¼Œåˆ†è¯å™¨ä¼šæ ¹æ®è¿™äº›å‚æ•°å°†è¯­è¨€å’Œä»»åŠ¡æ ‡è®°ä½œä¸ºè¾“å‡ºåºåˆ—å‰ç¼€ï¼š

```python
from transformers import WhisperTokenizer

tokenizer = WhisperTokenizer.from_pretrained("openai/whisper-small", language="Hindi", task="transcribe")
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹ Common Voice æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œç¼–ç å’Œè§£ç æ¥éªŒè¯åˆ†è¯å™¨æ˜¯å¦æ­£ç¡®ç¼–ç äº†å°åœ°è¯­å­—ç¬¦ã€‚åœ¨å¯¹å½•éŸ³æ–‡æœ¬è¿›è¡Œç¼–ç æ—¶ï¼Œæ ‡è®°å™¨å°†â€œç‰¹æ®Šæ ‡è®°â€æ·»åŠ åˆ°åºåˆ—çš„å¼€å¤´å’Œç»“å°¾ï¼Œå…¶ä¸­åŒ…æ‹¬æ–‡æœ¬çš„å¼€å§‹/ç»“å°¾ã€è¯­è¨€æ ‡è®°å’Œä»»åŠ¡æ ‡è®°ï¼ˆç”±ä¸Šä¸€æ­¥ä¸­çš„å‚æ•°æŒ‡å®šï¼‰ã€‚åœ¨è§£ç æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©â€œè·³è¿‡â€è¿™äº›ç‰¹æ®Šæ ‡è®°ï¼Œä»è€Œå…è®¸æˆ‘ä»¬ä»¥åŸå§‹è¾“å…¥å½¢å¼è¿”å›å­—ç¬¦ä¸²ï¼š

```python
input_str = common_voice["train"][0]["sentence"]
labels = tokenizer(input_str).input_ids
decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)
decoded_str = tokenizer.decode(labels, skip_special_tokens=True)

print(f"Input:                 {input_str}")
print(f"Decoded w/ special:    {decoded_with_special}")
print(f"Decoded w/out special: {decoded_str}")
print(f"Are equal:             {input_str == decoded_str}")
```
**æ‰“å°è¾“å‡ºï¼š**
```bash
Input:                 à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ à¤¨à¥‡ à¤¦à¥€ à¤¸à¤«à¤¾à¤ˆ
Decoded w/ special:    <|startoftranscript|><|hi|><|transcribe|><|notimestamps|>à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ à¤¨à¥‡ à¤¦à¥€ à¤¸à¤«à¤¾à¤ˆ<|endoftext|>
Decoded w/out special: à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ à¤¨à¥‡ à¤¦à¥€ à¤¸à¤«à¤¾à¤ˆ
Are equal:             True
```

### ç»„å»ºä¸€ä¸ª WhisperProcessor

ä¸ºäº†ç®€åŒ–ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸¤è€…*åŒ…è¿›*åˆ°ä¸€ä¸ª `WhisperProcessor` ç±»ã€‚è¯¥ç±»ç»§æ‰¿è‡ª `WhisperFeatureExtractor` åŠ `WhisperProcessor`ï¼Œå¯æ ¹æ®éœ€è¦ç”¨äºéŸ³é¢‘è¾“å…¥å’Œæ¨¡å‹é¢„æµ‹ã€‚è¿™æ ·ï¼Œåœ¨è®­ç»ƒæœŸé—´æˆ‘ä»¬åªéœ€è¦ä¿æŒä¸¤ä¸ªå¯¹è±¡ï¼š`processor` å’Œ `model`ï¼š

```python
from transformers import WhisperProcessor

processor = WhisperProcessor.from_pretrained("openai/whisper-small", language="Hindi", task="transcribe")
```

### å‡†å¤‡æ•°æ®
æˆ‘ä»¬æ‰“å° Common Voice æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼Œçœ‹çœ‹æ•°æ®æ˜¯ä»€ä¹ˆå½¢å¼çš„ï¼š

```python
print(common_voice["train"][0])
```
**æ‰“å°è¾“å‡ºï¼š**
```python
{'audio': {'path': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/607848c7e74a89a3b5225c0fa5ffb9470e39b7f11112db614962076a847f3abf/cv-corpus-11.0-2022-09-21/hi/clips/common_voice_hi_25998259.mp3', 
           'array': array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 9.6724887e-07,
       1.5334779e-06, 1.0415988e-06], dtype=float32), 
           'sampling_rate': 48000},
 'sentence': 'à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ à¤¨à¥‡ à¤¦à¥€ à¤¸à¤«à¤¾à¤ˆ'}
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æœ‰ä¸€ä¸ªä¸€ç»´è¾“å…¥éŸ³é¢‘æ•°ç»„åŠå…¶å¯¹åº”çš„å½•éŸ³æ–‡æœ¬ã€‚æˆ‘ä»¬å·²ç»å¤šæ¬¡è°ˆåˆ°äº†é‡‡æ ·ç‡çš„é‡è¦æ€§ï¼Œä»¥åŠå°†éŸ³é¢‘çš„é‡‡æ ·ç‡ä¸ Whisper æ¨¡å‹ (16kHz) çš„é‡‡æ ·ç‡ç›¸åŒ¹é…çš„é‡è¦æ€§ã€‚ç”±äºæˆ‘ä»¬çš„è¾“å…¥éŸ³é¢‘ä»¥ 48kHz é‡‡æ ·ï¼Œå› æ­¤åœ¨å°†å…¶é¦ˆé€ç»™ Whisper ç‰¹å¾æå–å™¨ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶*ä¸‹é‡‡æ ·*è‡³ 16kHzã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ `dataset` çš„ [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column) æ–¹æ³•å°†éŸ³é¢‘è¾“å…¥è½¬æ¢ä¸ºæ­£ç¡®çš„é‡‡æ ·ç‡ã€‚è¯¥æ“ä½œä¸ä¼šæ”¹å˜åŸéŸ³é¢‘æ•°æ®ï¼Œè€Œæ˜¯æŒ‡ç¤º `datasets` ï¼Œä»¥ä¾¿åœ¨ç¬¬ä¸€æ¬¡åŠ è½½éŸ³é¢‘æ ·æœ¬æ—¶*å³æ—¶åœ°*å¯¹å…¶è¿›è¡Œé‡æ–°é‡‡æ ·ï¼š

```python
from datasets import Audio

common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16000))
```

é‡æ–°åŠ è½½ Common Voice æ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªéŸ³é¢‘æ ·æœ¬ï¼Œå¯ä»¥çœ‹åˆ°å…¶å·²è¢«é‡é‡‡æ ·ä¸ºæ‰€éœ€çš„é‡‡æ ·ç‡ï¼š

```python
print(common_voice["train"][0])
```
**æ‰“å°è¾“å‡ºï¼š**
```python
{'audio': {'path': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/607848c7e74a89a3b5225c0fa5ffb9470e39b7f11112db614962076a847f3abf/cv-corpus-11.0-2022-09-21/hi/clips/common_voice_hi_25998259.mp3', 
           'array': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,
       -3.4206650e-07,  3.2979898e-07,  1.0042874e-06], dtype=float32),
           'sampling_rate': 16000},
 'sentence': 'à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ à¤¨à¥‡ à¤¦à¥€ à¤¸à¤«à¤¾à¤ˆ'}
```

é…·ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°é‡‡æ ·ç‡å·²ç»ä¸‹é‡‡æ ·åˆ° 16kHz äº†ã€‚æ•°ç»„å€¼ä¹Ÿå˜äº†ï¼Œå› ä¸ºç›¸è¾ƒè€Œè¨€ï¼Œä¹‹å‰çš„æ¯ 3 ä¸ªå¹…åº¦å€¼æ‰å¤§è‡´å¯¹åº”ç°åœ¨çš„ä¸€ä¸ªå¹…åº¦å€¼ã€‚

ç°åœ¨æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®ï¼š
1. è°ƒç”¨ `batch["audio"]` åŠ è½½å’Œé‡é‡‡æ ·éŸ³é¢‘æ•°æ®ã€‚å¦‚ä¸Šæ‰€è¿°ï¼ŒğŸ¤— Datasets ä¼šå³æ—¶æ‰§è¡Œä»»ä½•å¿…è¦çš„é‡é‡‡æ ·æ“ä½œã€‚
2. ä½¿ç”¨ç‰¹å¾æå–å™¨ä»ä¸€ç»´éŸ³é¢‘æ•°ç»„ä¸­è®¡ç®—å¯¹æ•°æ¢…å°”å£°è°±å›¾è¾“å…¥ç‰¹å¾ã€‚
3. é€šè¿‡ä½¿ç”¨ tokenizer å°†å½•éŸ³æ–‡æœ¬ç¼–ç ä¸º idã€‚

```python
def prepare_dataset(batch):
    # load and resample audio data from 48 to 16kHz
    audio = batch["audio"]

    # compute log-Mel input features from input audio array 
    batch["input_features"] = feature_extractor(audio["array"], sampling_rate=audio["sampling_rate"]).input_features[0]

    # encode target text to label ids 
    batch["labels"] = tokenizer(batch["sentence"]).input_ids
    return batch
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `dataset` çš„ `.map` æ–¹æ³•å°†æ•°æ®å‡†å¤‡å‡½æ•°åº”ç”¨äºæ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬ï¼š

```python
common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names["train"], num_proc=4)
```

å¥½äº†ï¼è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæ¯•ï¼æˆ‘ä»¬ç»§ç»­çœ‹çœ‹å¦‚ä½•ä½¿ç”¨è¿™äº›æ•°æ®æ¥å¾®è°ƒ Whisperã€‚

**æ³¨æ„**ï¼šç›®å‰ `datasets` åŒæ—¶ä½¿ç”¨ [`torchaudio`](https://pytorch.org/audio/stable/index.html) å’Œ [`librosa`](https://librosa.org /doc/latest/index.html) æ¥è¿›è¡ŒéŸ³é¢‘åŠ è½½å’Œé‡é‡‡æ ·ã€‚å¦‚æœä½ å®šåˆ¶ä¸€ä¸ªæ•°æ®åŠ è½½/é‡‡æ ·å‡½æ•°ï¼Œä½ å¯ä»¥ç›´æ¥é€šè¿‡ `"path"` åˆ—è·å–éŸ³é¢‘æ–‡ä»¶è·¯å¾„å¹¶å¿½ç•¥ `"audio"` åˆ—ã€‚

## è®­ç»ƒä¸è¯„ä¼°

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½æ•°æ®ï¼Œå¯ä»¥å¼€å§‹è¿›è¡Œè®­ç»ƒäº†ã€‚ [ğŸ¤— Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) å°†ä¸ºæˆ‘ä»¬å®Œæˆå¤§éƒ¨åˆ†ç¹é‡çš„å·¥ä½œã€‚æˆ‘ä»¬è¦åšçš„æœ‰ï¼š

- å®šä¹‰æ•°æ®æ•´ç†å™¨ï¼ˆdata collatorï¼‰ï¼šæ•°æ®æ•´ç†å™¨è·å–é¢„å¤„ç†åçš„æ•°æ®å¹¶è½¬æ¢ä¸º PyTorch å¼ é‡ã€‚

- è¯„ä¼°æŒ‡æ ‡ï¼šåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨[å•è¯é”™è¯¯ç‡ (word error rateï¼ŒWER)](https://huggingface.co/metrics/wer) æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹ã€‚æˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ª`compute_metrics`å‡½æ•°æ¥å¤„ç†æ­¤è®¡ç®—ã€‚

- åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ checkpointï¼šæˆ‘ä»¬éœ€è¦åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ checkpoint å¹¶æ­£ç¡®é…ç½®å®ƒä»¥è¿›è¡Œè®­ç»ƒã€‚

- å®šä¹‰è®­ç»ƒå‚æ•°ï¼šè¿™äº›å°†ç”± ğŸ¤— Trainer åœ¨æ„å»ºè®­ç»ƒè®¡åˆ’æ—¶ä½¿ç”¨ã€‚

å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒåï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æµ‹è¯•æ•°æ®å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œä»¥éªŒè¯æˆ‘ä»¬æ˜¯å¦å·²æ­£ç¡®è®­ç»ƒå®ƒä»¥å¯¹å°åœ°è¯­è¿›è¡Œè¯­éŸ³è¯†åˆ«ã€‚

### å®šä¹‰æ•°æ®æ•´ç†å™¨

åºåˆ—åˆ°åºåˆ—è¯­éŸ³æ¨¡å‹çš„æ•°æ®æ•´ç†å™¨ä¸å…¶ä»–åœºæ™¯æœ‰æ‰€ä¸åŒï¼Œå› ä¸º `input_features`å’Œ `labels` çš„å¤„ç†æ–¹æ³•æ˜¯ä¸åŒçš„ï¼š`input_features` å¿…é¡»ç”±ç‰¹å¾æå–å™¨å¤„ç†ï¼Œè€Œ `labels` ç”±åˆ†è¯å™¨å¤„ç†ã€‚

`input_features` å·²ç»å¡«å……è‡³ 30s å¹¶è½¬æ¢ä¸ºå›ºå®šç»´åº¦çš„å¯¹æ•°æ¢…å°”å£°è°±å›¾ï¼Œå› æ­¤æˆ‘ä»¬æ‰€è¦åšçš„å°±æ˜¯å°†å®ƒä»¬è½¬æ¢ä¸º PyTorch å¼ é‡ã€‚æˆ‘ä»¬ä½¿ç”¨å‚æ•°ä¸º `return_tensors=pt` çš„ç‰¹å¾æå–å™¨çš„ `.pad` æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚è¯·æ³¨æ„ï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–çš„å¡«å……ï¼Œå› ä¸ºè¾“å…¥æ˜¯å›ºå®šç»´åº¦çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦ç®€å•åœ°å°†`input_features` è½¬æ¢ä¸º PyTorch å¼ é‡ã€‚


å¦ä¸€æ–¹é¢ï¼Œ`labels` æ•°æ®å¹¶æœªå¡«å……ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨åˆ†è¯å™¨çš„ `.pad` æ–¹æ³•å°†åºåˆ—å¡«å……è‡³æœ¬ batch çš„æœ€å¤§é•¿åº¦ã€‚ç„¶åå°†å¡«å……æ ‡è®°æ›¿æ¢ä¸º `-100`ï¼Œä»¥ä½¿å®ƒä»¬**ä¸**æŸå¤±è®¡ç®—ã€‚ç„¶åæˆ‘ä»¬ä»æ ‡ç­¾åºåˆ—çš„å¼€å¤´å»æ‰ `SOT`ï¼Œç¨åè®­ç»ƒçš„æ—¶å€™å†åŠ ä¸Šå®ƒã€‚

æˆ‘ä»¬å¯ä»¥åˆ©ç”¨æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„ `WhisperProcessor` æ¥æ‰§è¡Œç‰¹å¾æå–å’Œåˆ†è¯æ“ä½œï¼š

```python
import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lengths and need different padding methods
        # first treat the audio inputs by simply returning torch tensors
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # get the tokenized label sequences
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        # pad the labels to max length
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # if bos token is appended in previous tokenization step,
        # cut bos token here as it's append later anyways
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch
```

æˆ‘ä»¬åˆå§‹åŒ–ä¸€ä¸‹æˆ‘ä»¬åˆšåˆšå®šä¹‰çš„æ•°æ®æ•´ç†å™¨ï¼š

```python
data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
```

### è¯„ä¼°æŒ‡æ ‡

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¯é”™è¯¯ç‡ (WER) æŒ‡æ ‡ï¼Œè¿™æ˜¯è¯„ä¼° ASR ç³»ç»Ÿçš„â€œæ ‡å‡†â€æŒ‡æ ‡ã€‚æœ‰å…³å…¶è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… WER [æ–‡æ¡£](https://huggingface.co/metrics/wer)ã€‚æˆ‘ä»¬å°†ä» ğŸ¤— Evaluate ä¸­åŠ è½½ WER æŒ‡æ ‡ï¼š

```python
import evaluate

metric = evaluate.load("wer")
```

ç„¶åæˆ‘ä»¬åªéœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æ¥å—æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹å¹¶è¿”å› WER æŒ‡æ ‡ã€‚è¿™ä¸ªåä¸º`compute_metrics` çš„å‡½æ•°é¦–å…ˆå°† `-100` æ›¿æ¢ä¸º `label_ids` ä¸­çš„`pad_token_id`ï¼ˆæ’¤æ¶ˆæˆ‘ä»¬åœ¨æ•°æ®æ•´ç†å™¨ä¸­åšçš„ï¼Œä»¥ä¾¿åœ¨æŸå¤±è®¡ç®—æ—¶æ­£ç¡®åœ°å¿½ç•¥æ‰å¡«å……æ ‡è®°ï¼‰ã€‚ç„¶åï¼Œå°†é¢„æµ‹åˆ°çš„ id å’Œ `label_ids` è§£ç ä¸ºå­—ç¬¦ä¸²ã€‚æœ€åï¼Œè®¡ç®—é¢„æµ‹å­—ç¬¦ä¸²å’ŒçœŸå®å­—ç¬¦ä¸²ä¹‹é—´çš„ WERï¼š

```python
def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # replace -100 with the pad_token_id
    label_ids[label_ids == -100] = tokenizer.pad_token_id

    # we do not want to group tokens when computing the metrics
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    wer = 100 * metric.compute(predictions=pred_str, references=label_str)

    return {"wer": wer}
```

### åŠ è½½é¢„è®­ç»ƒ checkpoint

ç°åœ¨æˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒçš„ Whisper `small` æ¨¡å‹çš„ checkpointã€‚åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ ğŸ¤— transformers å¾ˆè½»æ¾åœ°å®Œæˆè¿™ä¸€æ­¥ï¼


```python
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
```

åŸå§‹ Whisper æ¨¡å‹å«æœ‰åœ¨è‡ªå›å½’ç”Ÿæˆå¼€å§‹ä¹‹å‰å¼ºåˆ¶ä½œä¸ºæ¨¡å‹è¾“å‡ºçš„è¯å…ƒ ID([`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids)ï¼‰ã€‚è¿™äº›è¯å…ƒ ID ä»£è¡¨é›¶æ ·æœ¬ ASR ä»»åŠ¡è¯­ç§å’Œä»»åŠ¡ã€‚ç°åœ¨å› ä¸ºæˆ‘ä»¬æ˜¯å¯¹ç‰¹å®šçš„è¯­è¨€ï¼ˆå°åœ°è¯­ï¼‰å’Œä»»åŠ¡ï¼ˆè½¬å½•ï¼‰è¿›è¡Œå¾®è°ƒï¼Œæ‰€ä»¥æˆ‘ä»¬è¦å°† `forced_decoder_ids` è®¾ç½®ä¸º `None`ã€‚è¿˜æœ‰ä¸€äº›è¯å…ƒåœ¨ç”ŸæˆæœŸé—´è¢«æŠ‘åˆ¶äº†ï¼ˆ[`suppress_tokens`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.suppress_tokens)ï¼‰ã€‚è¿™äº›è¯å…ƒçš„å¯¹æ•°æ¦‚ç‡è¢«è®¾ç½®ä¸º `-inf`ï¼Œå› æ­¤å®ƒä»¬æ°¸è¿œä¸ä¼šè¢«é‡‡æ ·ã€‚æˆ‘ä»¬ä¼šå°† `suppress_tokens` è¦†ç›–ä¸ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œä¹Ÿå°±æ˜¯ä¸æŠ‘åˆ¶ä»»ä½•è¯å…ƒï¼š

```python
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
```

### å®šä¹‰è®­ç»ƒå‚æ•°

åœ¨æœ€åä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰ä¸è®­ç»ƒç›¸å…³çš„æ‰€æœ‰å‚æ•°ã€‚ä¸‹é¢è§£é‡Šäº†ä¸€éƒ¨åˆ†å‚æ•°ï¼š
- `output_dir`ï¼šä¿å­˜æ¨¡å‹æƒé‡çš„æœ¬åœ°ç›®å½•ã€‚è¿™ä¹Ÿå°†æ˜¯ [Hugging Face Hub](https://huggingface.co/) ä¸Šçš„æ¨¡å‹å­˜å‚¨åº“åç§°ã€‚
- `generation_max_length`ï¼šè¯„ä¼°æœŸé—´è‡ªå›å½’ç”Ÿæˆçš„æœ€å¤§è¯å…ƒæ•°ã€‚
- `save_steps`ï¼šåœ¨è®­ç»ƒæœŸé—´ï¼Œæ¯ `save_steps` æ­¥ä¿å­˜ä¸€æ¬¡ä¸­é—´ checkpoint å¹¶å¼‚æ­¥ä¸Šä¼ åˆ° Hubã€‚
- `eval_steps`ï¼šåœ¨è®­ç»ƒæœŸé—´ï¼Œæ¯ `eval_steps` æ­¥å¯¹ä¸­é—´ checkpoint è¿›è¡Œä¸€æ¬¡è¯„ä¼°ã€‚
- `report_to`ï¼šä¿å­˜è®­ç»ƒæ—¥å¿—çš„ä½ç½®ã€‚æ”¯æŒçš„å¹³å°æœ‰ `azure_ml`ã€`comet_ml`ã€`mlflow`ã€`neptune`ã€`tensorboard` ä»¥åŠ `wandb`ã€‚é€‰æ‹©ä½ æœ€å–œæ¬¢çš„æˆ–ä½¿ç”¨ç¼ºçœçš„ `tensorboard` ä¿å­˜è‡³ Hubã€‚

æœ‰å…³å…¶ä»–è®­ç»ƒå‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… Seq2SeqTrainingArguments [æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)ã€‚

```python
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-small-hi",  # change to a repo name of your choice
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=4000,
    gradient_checkpointing=True,
    fp16=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=8,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=True,
)
```

**æ³¨æ„**ï¼šå¦‚æœä¸æƒ³å°†æ¨¡å‹ checkpoint ä¸Šä¼ åˆ° Hubï¼Œä½ éœ€è¦è®¾ç½® `push_to_hub=False`ã€‚

æˆ‘ä»¬å¯ä»¥å°†è®­ç»ƒå‚æ•°è¿åŒæˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®é›†ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ä¸€èµ·ä¼ ç»™ ğŸ¤— Trainerï¼š

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=common_voice["train"],
    eval_dataset=common_voice["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor.feature_extractor,
)
```

æœ‰äº†è¿™äº›ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼

### è®­ç»ƒ
è¦å¯åŠ¨è®­ç»ƒï¼Œåªéœ€æ‰§è¡Œï¼š

```python
trainer.train()
```

è®­ç»ƒå¤§çº¦éœ€è¦ 5-10 ä¸ªå°æ—¶ï¼Œå…·ä½“å–å†³äºä½ çš„ GPU æˆ–åˆ†é…ç»™ Google Colab çš„ GPUã€‚æ ¹æ® GPU çš„æƒ…å†µï¼Œä½ å¯èƒ½ä¼šåœ¨å¼€å§‹è®­ç»ƒæ—¶é‡åˆ° CUDA `å†…å­˜è€—å°½`é”™è¯¯ã€‚æ­¤æ—¶ï¼Œä½ å¯ä»¥å°† `per_device_train_batch_size` é€æ¬¡å‡å°‘ 2 å€ï¼Œå¹¶ä½¿ç”¨ [`gradient_accumulation_steps`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.gradient_accumulation_steps) è¿›è¡Œè¡¥å¿ã€‚

**æ‰“å°è¾“å‡ºï¼š**

| æ­¥æ•° | è®­ç»ƒæŸå¤± | è½®æ•° | éªŒè¯æŸå¤± |  WER  |
|:----:|:-------------:|:-----:|:---------------:|:-----:|
| 1000 |    0.1011     | 2.44  |     0.3075      | 34.63 |
| 2000 |    0.0264     | 4.89  |     0.3558      | 33.13 |
| 3000 |    0.0025     | 7.33  |     0.4214      | 32.59 |
| 4000 |    0.0006     | 9.78  |     0.4519      | 32.01 |
| 5000 |    0.0002     | 12.22 |     0.4679      | 32.10 |

Our best WER is 32.0% - not bad for 8h of training data! The big question is how this compares to other ASR systems. For that, we can view the [`hf-speech-bench`](https://huggingface.co/spaces/huggingface/hf-speech-bench), a leaderboard that categorises models by language and dataset, and subsequently ranks them according to their WER.

æœ€ä½³ WER æ˜¯ 32.0% â€”â€” å¯¹äº 8 å°æ—¶çš„è®­ç»ƒæ•°æ®æ¥è¯´è¿˜ä¸é”™ï¼é‚£ä¸å…¶ä»– ASR ç³»ç»Ÿç›¸æ¯”ï¼Œè¿™ä¸ªè¡¨ç°åˆ°åº•å¤„äºä»€ä¹ˆæ°´å¹³ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ [`hf-speech-bench`](https://huggingface.co/spaces/huggingface/hf-speech-bench)ï¼Œè¿™æ˜¯ä¸€ä¸ªæŒ‰è¯­ç§å’Œæ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œåˆ†é—¨åˆ«ç±»ï¼Œå¹¶æŒ‰ç±»åˆ«æ ¹æ®å®ƒä»¬çš„ WER è¿›è¡Œæ’åçš„æ’è¡Œæ¦œã€‚

<figure>
<img src="assets/111_fine_tune_whisper/hf_speech_bench.jpg" alt="Trulli" style="width:100%">
</figure>

æˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹æ˜¾è‘—æé«˜äº† Whisper `small` checkpoint çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä¹Ÿçªå‡ºå±•ç¤ºäº† Whisper å¼ºå¤§çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚

å½“æˆ‘ä»¬å°†è®­ç»ƒç»“æœæ¨é€åˆ° Hub æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªåŠ¨å°† checkpoint æäº¤åˆ°æ’è¡Œæ¦œ â€”â€” åªéœ€é…ç½®é€‚å½“çš„å…³é”®å­—å‚æ•° (key-word argumentsï¼Œkwargs)ã€‚ä½ è¿˜å¯ä»¥æ›´æ”¹è¿™äº›å€¼ä»¥ç›¸åº”åœ°åŒ¹é…ä½ çš„æ•°æ®é›†ã€è¯­ç§å’Œæ¨¡å‹åç§°ï¼š

```python
kwargs = {
    "dataset_tags": "mozilla-foundation/common_voice_11_0",
    "dataset": "Common Voice 11.0",  # a 'pretty' name for the training dataset
    "dataset_args": "config: hi, split: test",
    "language": "hi",
    "model_name": "Whisper Small Hi - Sanchit Gandhi",  # a 'pretty' name for your model
    "finetuned_from": "openai/whisper-small",
    "tasks": "automatic-speech-recognition",
    "tags": "hf-asr-leaderboard",
}
```

ç°åœ¨å¯ä»¥å°†è®­ç»ƒç»“æœä¸Šä¼ åˆ° Hubäº†ã€‚ä¸ºæ­¤ï¼Œè¯·æ‰§è¡Œ `push_to_hub` å‘½ä»¤ï¼š

```python
trainer.push_to_hub(**kwargs)
```

æ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨ Hub ä¸Šçš„é“¾æ¥ä¸ä»»ä½•äººå…±äº«æ­¤æ¨¡å‹ã€‚ä»–ä»¬è¿˜å¯ä»¥ä½¿ç”¨æ ‡è¯†ç¬¦`"your-username/the-name-you-picked"`åŠ è½½å®ƒï¼Œä¾‹å¦‚ï¼š

```python
from transformers import WhisperForConditionalGeneration, WhisperProcessor

model = WhisperForConditionalGeneration.from_pretrained("sanchit-gandhi/whisper-small-hi")
processor = WhisperProcessor.from_pretrained("sanchit-gandhi/whisper-small-hi")
```

è™½ç„¶å¾®è°ƒæ¨¡å‹åœ¨ Common Voice Hindi æµ‹è¯•æ•°æ®ä¸Šäº§ç”Ÿäº†ä»¤äººæ»¡æ„çš„ç»“æœï¼Œä½†å®ƒç»ä¸æ˜¯æœ€ä¼˜çš„ã€‚æœ¬æ–‡çš„ç›®çš„æ˜¯æ¼”ç¤ºå¦‚ä½•åœ¨ä»»ä½•å¤šè¯­ç§ ASR æ•°æ®é›†ä¸Šå¾®è°ƒé¢„è®­ç»ƒçš„ Whisper checkpointã€‚é€šè¿‡ä¼˜åŒ–è®­ç»ƒè¶…å‚ï¼ˆä¾‹å¦‚ *learning rate* å’Œ *dropout*ï¼‰å¹¶ä½¿ç”¨æ›´å¤§çš„é¢„è®­ç»ƒ checkpointï¼ˆ`medium` æˆ– `large`ï¼‰å¯èƒ½ä¼šæå‡æ•ˆæœã€‚

### æ„å»ºæ¼”ç¤ºåº”ç”¨

ç°åœ¨æˆ‘ä»¬å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªæ¼”ç¤ºæ¥å±•ç¤ºå…¶ ASR åŠŸèƒ½ï¼æˆ‘ä»¬å°†ä½¿ç”¨ ğŸ¤— Transformers `pipeline`ï¼Œå®ƒå°†å®Œæˆæ•´ä¸ª ASR æµæ°´çº¿ï¼Œä»å¯¹éŸ³é¢‘è¾“å…¥è¿›è¡Œé¢„å¤„ç†åˆ°å¯¹æ¨¡å‹é¢„æµ‹è¾“å‡ºè¿›è¡Œè§£ç ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ [Gradio](https://www.gradio.app) æ„å»ºæˆ‘ä»¬çš„äº¤äº’å¼æ¼”ç¤ºã€‚ Gradio å¯ä»¥è¯´æ˜¯æ„å»ºæœºå™¨å­¦ä¹ æ¼”ç¤ºçš„æœ€ç›´æ¥çš„æ–¹å¼ï¼›ä½¿ç”¨ Gradioï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…æ„å»ºä¸€ä¸ªæ¼”ç¤ºï¼

è¿è¡Œä¸‹é¢çš„ç¤ºä¾‹å°†ç”Ÿæˆä¸€ä¸ª Gradio æ¼”ç¤ºåº”ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—æœºçš„éº¦å…‹é£å½•åˆ¶è¯­éŸ³å¹¶å°†å…¶é¦ˆé€ç»™æˆ‘ä»¬å¾®è°ƒçš„ Whisper æ¨¡å‹ä»¥è½¬å½•å‡ºç›¸åº”çš„æ–‡æœ¬ï¼š

```python
from transformers import pipeline
import gradio as gr

pipe = pipeline(model="sanchit-gandhi/whisper-small-hi")  # change to "your-username/the-name-you-picked"

def transcribe(audio):
    text = pipe(audio)["text"]
    return text

iface = gr.Interface(
    fn=transcribe, 
    inputs=gr.Audio(source="microphone", type="filepath"), 
    outputs="text",
    title="Whisper Small Hindi",
    description="Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.",
)

iface.launch()
```

## ç»“æŸè¯­

é€šè¿‡æœ¬æ–‡ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ ğŸ¤— Datasetsã€Transformers å’Œ Hugging Face Hub ä¸€æ­¥æ­¥ä¸ºå¤šè¯­ç§ ASR å¾®è°ƒä¸€ä¸ª Whisper æ¨¡å‹ã€‚å¦‚æœä½ æƒ³è‡ªå·±è¯•è¯•å¾®è°ƒä¸€ä¸ªï¼Œè¯·å‚é˜… [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb)ã€‚å¦‚æœä½ æœ‰å…´è¶£é’ˆå¯¹è‹±è¯­å’Œå¤šè¯­ç§ ASR å¾®è°ƒä¸€ä¸ªå…¶å®ƒçš„ Transformers æ¨¡å‹ï¼Œè¯·åŠ¡å¿…æŸ¥çœ‹ [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognitionï¼‰ã€‚

> è‹±æ–‡åŸæ–‡: <url> https://huggingface.co/blog/fine-tune-whisper </url>
> åŸæ–‡ä½œè€…ï¼šSanchit Gandhi
> è¯‘è€…: Matrix Yao (å§šä¼Ÿå³°)ï¼Œè‹±ç‰¹å°”æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œå·¥ä½œæ–¹å‘ä¸º transformer-family æ¨¡å‹åœ¨å„æ¨¡æ€æ•°æ®ä¸Šçš„åº”ç”¨åŠå¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒæ¨ç†ã€‚
